@article{Agostinelli2023,
  title = {{{MusicLM}}: {{Generating Music From Text}}},
  shorttitle = {{{MusicLM}}},
  author = {Agostinelli, Andrea and Denk, Timo I. and Borsos, Zal{\'a}n and Engel, Jesse and Verzetti, Mauro and Caillon, Antoine and Huang, Qingqing and Jansen, Aren and Roberts, Adam and Tagliasacchi, Marco},
  year = {2023},
  journal = {arXiv preprint arXiv:2301.11325},
  eprint = {2301.11325},
  archiveprefix = {arxiv},
  keywords = {notion},
  file = {G\:\\내 드라이브\\Papers\\MusicLM - Generating Music From Text_2023_Agostinelli et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\3IJ7HPYE\\2301.html}
}

@article{Ahlmann-Eltze2020,
  title = {{{proDA}}: Probabilistic Dropout Analysis for Identifying Differentially Abundant Proteins in Label-Free Mass Spectrometry},
  shorttitle = {{{proDA}}},
  author = {{Ahlmann-Eltze}, Constantin and Anders, Simon},
  year = {2020},
  journal = {Biorxiv},
  pages = {661496},
  publisher = {{Cold Spring Harbor Laboratory}},
  keywords = {notion},
  file = {G\:\\내 드라이브\\Papers\\proDA - probabilistic dropout analysis for identifying differentially abundant_2020_Ahlmann-Eltze et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\YEHE4EBX\\661496v2.html}
}

@article{Ajakan2014,
  title = {Domain-Adversarial Neural Networks},
  author = {Ajakan, Hana and Germain, Pascal and Larochelle, Hugo and Laviolette, Fran{\c c}ois and Marchand, Mario},
  year = {2014},
  journal = {arXiv preprint arXiv:1412.4446},
  eprint = {1412.4446},
  archiveprefix = {arxiv},
  keywords = {notion},
  file = {G\:\\내 드라이브\\Papers\\Domain-adversarial neural networks_2014_Ajakan et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\92Y43G3K\\1412.html}
}

@article{Bahdanau2014,
  title = {Neural Machine Translation by Jointly Learning to Align and Translate},
  author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  year = {2014},
  journal = {arXiv preprint arXiv:1409.0473},
  eprint = {1409.0473},
  archiveprefix = {arxiv},
  keywords = {notion},
  file = {G\:\\내 드라이브\\Papers\\Neural machine translation by jointly learning to align and translate_2014_Bahdanau et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\K4X2PCDD\\1409.html}
}

@article{Bao2022,
  title = {Analytic-Dpm: An Analytic Estimate of the Optimal Reverse Variance in Diffusion Probabilistic Models},
  shorttitle = {Analytic-Dpm},
  author = {Bao, Fan and Li, Chongxuan and Zhu, Jun and Zhang, Bo},
  year = {2022},
  journal = {arXiv preprint arXiv:2201.06503},
  eprint = {2201.06503},
  archiveprefix = {arxiv},
  keywords = {notion},
  file = {G\:\\내 드라이브\\Papers\\Analytic-dpm - an analytic estimate of the optimal reverse variance in diffusion_2022_Bao et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\MQXRRZMY\\2201.html}
}

@inproceedings{Barron2021,
  title = {Mip-Nerf: {{A}} Multiscale Representation for Anti-Aliasing Neural Radiance Fields},
  shorttitle = {Mip-Nerf},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}}},
  author = {Barron, Jonathan T. and Mildenhall, Ben and Tancik, Matthew and Hedman, Peter and {Martin-Brualla}, Ricardo and Srinivasan, Pratul P.},
  year = {2021},
  pages = {5855--5864},
  keywords = {notion},
  file = {G\:\\내 드라이브\\Papers\\Mip-nerf - A multiscale representation for anti-aliasing neural radiance fields_2021_Barron et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\7I8H49VK\\Barron_Mip-NeRF_A_Multiscale_Representation_for_Anti-Aliasing_Neural_Radiance_Fields_ICCV_2021_.html}
}

@article{Blei2003,
  title = {Latent Dirichlet Allocation},
  author = {Blei, David M. and Ng, Andrew Y. and Jordan, Michael I.},
  year = {2003},
  journal = {Journal of machine Learning research},
  volume = {3},
  number = {Jan},
  pages = {993--1022},
  urldate = {2023-10-04},
  keywords = {⛔ No DOI found},
  file = {G:\내 드라이브\Papers\Latent dirichlet allocation_2003_Blei et al.pdf}
}

@article{Brown2020,
  title = {Language Models Are Few-Shot Learners},
  author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D. and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda},
  year = {2020},
  journal = {Advances in neural information processing systems},
  volume = {33},
  pages = {1877--1901},
  keywords = {notion},
  file = {G\:\\내 드라이브\\Papers\\Language models are few-shot learners_2020_Brown et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\JKXZ5I7I\\1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html}
}

@misc{Cai2017,
  title = {Deep {{Learning}} with {{Low Precision}} by {{Half-wave Gaussian Quantization}}},
  author = {Cai, Zhaowei and He, Xiaodong and Sun, Jian and Vasconcelos, Nuno},
  year = {2017},
  month = feb,
  number = {arXiv:1702.00953},
  eprint = {1702.00953},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1702.00953},
  urldate = {2023-09-18},
  abstract = {The problem of quantizing the activations of a deep neural network is considered. An examination of the popular binary quantization approach shows that this consists of approximating a classical non-linearity, the hyperbolic tangent, by two functions: a piecewise constant sign function, which is used in feedforward network computations, and a piecewise linear hard tanh function, used in the backpropagation step during network learning. The problem of approximating the ReLU non-linearity, widely used in the recent deep learning literature, is then considered. An half-wave Gaussian quantizer (HWGQ) is proposed for forward approximation and shown to have efficient implementation, by exploiting the statistics of of network activations and batch normalization operations commonly used in the literature. To overcome the problem of gradient mismatch, due to the use of different forward and backward approximations, several piece-wise backward approximators are then investigated. The implementation of the resulting quantized network, denoted as HWGQ-Net, is shown to achieve much closer performance to full precision networks, such as AlexNet, ResNet, GoogLeNet and VGG-Net, than previously available low-precision networks, with 1-bit binary weights and 2-bit quantized activations.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {G\:\\내 드라이브\\Papers\\Deep Learning with Low Precision by Half-wave Gaussian Quantization_2017_Cai et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\N3J7GGIK\\1702.html}
}

@article{Canny1986,
  title = {A Computational Approach to Edge Detection},
  author = {Canny, John},
  year = {1986},
  journal = {IEEE Transactions on pattern analysis and machine intelligence},
  number = {6},
  pages = {679--698},
  publisher = {{Ieee}},
  keywords = {notion},
  file = {C:\Users\giwon\Zotero\storage\QPCU7FAL\4767851.html}
}

@article{Chen2017,
  title = {Rethinking Atrous Convolution for Semantic Image Segmentation},
  author = {Chen, Liang-Chieh and Papandreou, George and Schroff, Florian and Adam, Hartwig},
  year = {2017},
  journal = {arXiv preprint arXiv:1706.05587},
  eprint = {1706.05587},
  archiveprefix = {arxiv},
  keywords = {notion},
  file = {G\:\\내 드라이브\\Papers\\Rethinking atrous convolution for semantic image segmentation_2017_Chen et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\4YTG5VE2\\1706.html}
}

@article{Chen2017a,
  title = {Deeplab: {{Semantic}} Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected Crfs},
  shorttitle = {Deeplab},
  author = {Chen, Liang-Chieh and Papandreou, George and Kokkinos, Iasonas and Murphy, Kevin and Yuille, Alan L.},
  year = {2017},
  journal = {IEEE transactions on pattern analysis and machine intelligence},
  volume = {40},
  number = {4},
  pages = {834--848},
  publisher = {{IEEE}},
  keywords = {notion},
  file = {G\:\\내 드라이브\\Papers\\Deeplab - Semantic image segmentation with deep convolutional nets, atrous_2017_Chen et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\FEENX47X\\7913730.html}
}

@misc{Chen2021,
  title = {Decision {{Transformer}}: {{Reinforcement Learning}} via {{Sequence Modeling}}},
  shorttitle = {Decision {{Transformer}}},
  author = {Chen, Lili and Lu, Kevin and Rajeswaran, Aravind and Lee, Kimin and Grover, Aditya and Laskin, Michael and Abbeel, Pieter and Srinivas, Aravind and Mordatch, Igor},
  year = {2021},
  month = jun,
  number = {arXiv:2106.01345},
  eprint = {2106.01345},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2106.01345},
  urldate = {2023-02-12},
  abstract = {We introduce a framework that abstracts Reinforcement Learning (RL) as a sequence modeling problem. This allows us to draw upon the simplicity and scalability of the Transformer architecture, and associated advances in language modeling such as GPT-x and BERT. In particular, we present Decision Transformer, an architecture that casts the problem of RL as conditional sequence modeling. Unlike prior approaches to RL that fit value functions or compute policy gradients, Decision Transformer simply outputs the optimal actions by leveraging a causally masked Transformer. By conditioning an autoregressive model on the desired return (reward), past states, and actions, our Decision Transformer model can generate future actions that achieve the desired return. Despite its simplicity, Decision Transformer matches or exceeds the performance of state-of-the-art model-free offline RL baselines on Atari, OpenAI Gym, and Key-to-Door tasks.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,notion},
  file = {G\:\\내 드라이브\\Papers\\Decision Transformer - Reinforcement Learning via Sequence Modeling_2021_Chen et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\JGZEWKN2\\2106.html}
}

@misc{Chen2022,
  title = {Activating {{More Pixels}} in {{Image Super-Resolution Transformer}}},
  author = {Chen, Xiangyu and Wang, Xintao and Zhou, Jiantao and Dong, Chao},
  year = {2022},
  month = may,
  number = {arXiv:2205.04437},
  eprint = {2205.04437},
  primaryclass = {cs, eess},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2205.04437},
  urldate = {2023-01-29},
  abstract = {Transformer-based methods have shown impressive performance in low-level vision tasks, such as image super-resolution. However, we find that these networks can only utilize a limited spatial range of input information through attribution analysis. This implies that the potential of Transformer is still not fully exploited in existing networks. In order to activate more input pixels for reconstruction, we propose a novel Hybrid Attention Transformer (HAT). It combines channel attention and self-attention schemes, thus making use of their complementary advantages. Moreover, to better aggregate the cross-window information, we introduce an overlapping cross-attention module to enhance the interaction between neighboring window features. In the training stage, we additionally propose a same-task pre-training strategy to bring further improvement. Extensive experiments show the effectiveness of the proposed modules, and the overall method significantly outperforms the state-of-the-art methods by more than 1dB. Codes and models will be available at https://github.com/chxy95/HAT.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Electrical Engineering and Systems Science - Image and Video Processing,notion},
  file = {G\:\\내 드라이브\\Papers\\Activating More Pixels in Image Super-Resolution Transformer_2022_Chen et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\TN8Q5TPQ\\2205.html}
}

@article{Chen2022a,
  title = {Vision {{Transformer Adapter}} for {{Dense Predictions}}},
  author = {Chen, Zhe and Duan, Yuchen and Wang, Wenhai and He, Junjun and Lu, Tong and Dai, Jifeng and Qiao, Yu},
  year = {2022},
  journal = {arXiv preprint arXiv:2205.08534},
  eprint = {2205.08534},
  archiveprefix = {arxiv},
  keywords = {notion},
  file = {G\:\\내 드라이브\\Papers\\Vision Transformer Adapter for Dense Predictions_2022_Chen et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\8XWVS5TY\\2205.html}
}

@misc{Chen2023,
  title = {Symbolic {{Discovery}} of {{Optimization Algorithms}}},
  author = {Chen, Xiangning and Liang, Chen and Huang, Da and Real, Esteban and Wang, Kaiyuan and Liu, Yao and Pham, Hieu and Dong, Xuanyi and Luong, Thang and Hsieh, Cho-Jui and Lu, Yifeng and Le, Quoc V.},
  year = {2023},
  month = feb,
  number = {arXiv:2302.06675},
  eprint = {2302.06675},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2302.06675},
  urldate = {2023-02-22},
  abstract = {We present a method to formulate algorithm discovery as program search, and apply it to discover optimization algorithms for deep neural network training. We leverage efficient search techniques to explore an infinite and sparse program space. To bridge the large generalization gap between proxy and target tasks, we also introduce program selection and simplification strategies. Our method discovers a simple and effective optimization algorithm, \${\textbackslash}textbf\{Lion\}\$ (\${\textbackslash}textit\{Evo\${\textbackslash}textbf\{L\}\$ved S\${\textbackslash}textbf\{i\}\$gn M\${\textbackslash}textbf\{o\}\$me\${\textbackslash}textbf\{n\}\$tum\}\$). It is more memory-efficient than Adam as it only keeps track of the momentum. Different from adaptive optimizers, its update has the same magnitude for each parameter calculated through the sign operation. We compare Lion with widely used optimizers, such as Adam and Adafactor, for training a variety of models on different tasks. On image classification, Lion boosts the accuracy of ViT by up to 2\% on ImageNet and saves up to 5x the pre-training compute on JFT. On vision-language contrastive learning, we achieve 88.3\% \${\textbackslash}textit\{zero-shot\}\$ and 91.1\% \${\textbackslash}textit\{fine-tuning\}\$ accuracy on ImageNet, surpassing the previous best results by 2\% and 0.1\%, respectively. On diffusion models, Lion outperforms Adam by achieving a better FID score and reducing the training compute by up to 2.3x. For autoregressive, masked language modeling, and fine-tuning, Lion exhibits a similar or better performance compared to Adam. Our analysis of Lion reveals that its performance gain grows with the training batch size. It also requires a smaller learning rate than Adam due to the larger norm of the update produced by the sign function. Additionally, we examine the limitations of Lion and identify scenarios where its improvements are small or not statistically significant. The implementation of Lion is publicly available.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,notion},
  file = {G\:\\내 드라이브\\Papers\\Symbolic Discovery of Optimization Algorithms_2023_Chen et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\HWSV6A8V\\2302.html}
}

@misc{Choi2018,
  title = {{{PACT}}: {{Parameterized Clipping Activation}} for {{Quantized Neural Networks}}},
  shorttitle = {{{PACT}}},
  author = {Choi, Jungwook and Wang, Zhuo and Venkataramani, Swagath and Chuang, Pierce I.-Jen and Srinivasan, Vijayalakshmi and Gopalakrishnan, Kailash},
  year = {2018},
  month = jul,
  number = {arXiv:1805.06085},
  eprint = {1805.06085},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1805.06085},
  urldate = {2023-09-18},
  abstract = {Deep learning algorithms achieve high classification accuracy at the expense of significant computation cost. To address this cost, a number of quantization schemes have been proposed - but most of these techniques focused on quantizing weights, which are relatively smaller in size compared to activations. This paper proposes a novel quantization scheme for activations during training - that enables neural networks to work well with ultra low precision weights and activations without any significant accuracy degradation. This technique, PArameterized Clipping acTivation (PACT), uses an activation clipping parameter \${\textbackslash}alpha\$ that is optimized during training to find the right quantization scale. PACT allows quantizing activations to arbitrary bit precisions, while achieving much better accuracy relative to published state-of-the-art quantization schemes. We show, for the first time, that both weights and activations can be quantized to 4-bits of precision while still achieving accuracy comparable to full precision networks across a range of popular models and datasets. We also show that exploiting these reduced-precision computational units in hardware can enable a super-linear improvement in inferencing performance due to a significant reduction in the area of accelerator compute engines coupled with the ability to retain the quantized model and activation data in on-chip memories.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  file = {G\:\\내 드라이브\\Papers\\PACT - Parameterized Clipping Activation for Quantized Neural Networks_2018_Choi et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\JDICSWMH\\1805.html}
}

@misc{Chung2014,
  title = {Empirical {{Evaluation}} of {{Gated Recurrent Neural Networks}} on {{Sequence Modeling}}},
  author = {Chung, Junyoung and Gulcehre, Caglar and Cho, KyungHyun and Bengio, Yoshua},
  year = {2014},
  month = dec,
  number = {arXiv:1412.3555},
  eprint = {1412.3555},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1412.3555},
  urldate = {2023-05-18},
  abstract = {In this paper we compare different types of recurrent units in recurrent neural networks (RNNs). Especially, we focus on more sophisticated units that implement a gating mechanism, such as a long short-term memory (LSTM) unit and a recently proposed gated recurrent unit (GRU). We evaluate these recurrent units on the tasks of polyphonic music modeling and speech signal modeling. Our experiments revealed that these advanced recurrent units are indeed better than more traditional recurrent units such as tanh units. Also, we found GRU to be comparable to LSTM.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,notion},
  file = {G\:\\내 드라이브\\Papers\\Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling_2014_Chung et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\DFTSC4WI\\1412.html}
}

@article{Church2017,
  title = {{{Word2Vec}}},
  author = {Church, Kenneth Ward},
  year = {2017},
  journal = {Natural Language Engineering},
  volume = {23},
  number = {1},
  pages = {155--162},
  publisher = {{Cambridge University Press}},
  keywords = {notion},
  file = {G\:\\내 드라이브\\Papers\\Word2Vec_2017_Church.pdf;C\:\\Users\\giwon\\Zotero\\storage\\4BCMM5WC\\B84AE4446BD47F48847B4904F0B36E0B.html}
}

@misc{Courbariaux2016,
  title = {{{BinaryConnect}}: {{Training Deep Neural Networks}} with Binary Weights during Propagations},
  shorttitle = {{{BinaryConnect}}},
  author = {Courbariaux, Matthieu and Bengio, Yoshua and David, Jean-Pierre},
  year = {2016},
  month = apr,
  number = {arXiv:1511.00363},
  eprint = {1511.00363},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1511.00363},
  urldate = {2023-09-04},
  abstract = {Deep Neural Networks (DNN) have achieved state-of-the-art results in a wide range of tasks, with the best results obtained with large training sets and large models. In the past, GPUs enabled these breakthroughs because of their greater computational speed. In the future, faster computation at both training and test time is likely to be crucial for further progress and for consumer applications on low-power devices. As a result, there is much interest in research and development of dedicated hardware for Deep Learning (DL). Binary weights, i.e., weights which are constrained to only two possible values (e.g. -1 or 1), would bring great benefits to specialized DL hardware by replacing many multiply-accumulate operations by simple accumulations, as multipliers are the most space and power-hungry components of the digital implementation of neural networks. We introduce BinaryConnect, a method which consists in training a DNN with binary weights during the forward and backward propagations, while retaining precision of the stored weights in which gradients are accumulated. Like other dropout schemes, we show that BinaryConnect acts as regularizer and we obtain near state-of-the-art results with BinaryConnect on the permutation-invariant MNIST, CIFAR-10 and SVHN.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,notion},
  file = {G\:\\내 드라이브\\Papers\\BinaryConnect - Training Deep Neural Networks with binary weights during_2016_Courbariaux et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\3LVAEWEU\\1511.html}
}

@article{Cranmer2020,
  title = {Discovering Symbolic Models from Deep Learning with Inductive Biases},
  author = {Cranmer, Miles and Sanchez Gonzalez, Alvaro and Battaglia, Peter and Xu, Rui and Cranmer, Kyle and Spergel, David and Ho, Shirley},
  year = {2020},
  journal = {Advances in Neural Information Processing Systems},
  volume = {33},
  pages = {17429--17442},
  keywords = {notion},
  file = {G\:\\내 드라이브\\Papers\\Discovering symbolic models from deep learning with inductive biases_2020_Cranmer et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\NMGXRHA2\\c9f2f917078bd2db12f23c3b413d9cba-Abstract.html}
}

@misc{Defazio2023,
  title = {Learning-{{Rate-Free Learning}} by {{D-Adaptation}}},
  author = {Defazio, Aaron and Mishchenko, Konstantin},
  year = {2023},
  month = jan,
  number = {arXiv:2301.07733},
  eprint = {2301.07733},
  primaryclass = {cs, math, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2301.07733},
  urldate = {2023-02-22},
  abstract = {The speed of gradient descent for convex Lipschitz functions is highly dependent on the choice of learning rate. Setting the learning rate to achieve the optimal convergence rate requires knowing the distance D from the initial point to the solution set. In this work, we describe a single-loop method, with no back-tracking or line searches, which does not require knowledge of \$D\$ yet asymptotically achieves the optimal rate of convergence for the complexity class of convex Lipschitz functions. Our approach is the first parameter-free method for this class without additional multiplicative log factors in the convergence rate. We present extensive experiments for SGD and Adam variants of our method, where the method automatically matches hand-tuned learning rates across more than a dozen diverse machine learning problems, including large-scale vision and language problems. Our method is practical, efficient and requires no additional function value or gradient evaluations each step. An open-source implementation is available (https://github.com/facebookresearch/dadaptation).},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Mathematics - Optimization and Control,notion,Statistics - Machine Learning},
  file = {G\:\\내 드라이브\\Papers\\Learning-Rate-Free Learning by D-Adaptation_2023_Defazio et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\9CS75KVQ\\2301.html}
}

@article{Dhariwal2021,
  title = {Diffusion Models Beat Gans on Image Synthesis},
  author = {Dhariwal, Prafulla and Nichol, Alexander},
  year = {2021},
  journal = {Advances in Neural Information Processing Systems},
  volume = {34},
  pages = {8780--8794},
  keywords = {notion},
  file = {G\:\\내 드라이브\\Papers\\Diffusion models beat gans on image synthesis_2021_Dhariwal et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\2E5IPGZ5\\49ad23d1ec9fa4bd8d77d02681df5cfa-Abstract.html}
}

@inproceedings{Dong2016,
  title = {Accelerating the Super-Resolution Convolutional Neural Network},
  booktitle = {Computer {{Vision}}{\textendash}{{ECCV}} 2016: 14th {{European Conference}}, {{Amsterdam}}, {{The Netherlands}}, {{October}} 11-14, 2016, {{Proceedings}}, {{Part II}} 14},
  author = {Dong, Chao and Loy, Chen Change and Tang, Xiaoou},
  year = {2016},
  pages = {391--407},
  publisher = {{Springer}},
  keywords = {⛔ No DOI found,notion},
  file = {G\:\\내 드라이브\\Papers\\Accelerating the super-resolution convolutional neural network_2016_Dong et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\PGA4F3XK\\978-3-319-46475-6_25.html}
}

@article{Dosovitskiy2020,
  title = {An Image Is Worth 16x16 Words: {{Transformers}} for Image Recognition at Scale},
  shorttitle = {An Image Is Worth 16x16 Words},
  author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain},
  year = {2020},
  journal = {arXiv preprint arXiv:2010.11929},
  eprint = {2010.11929},
  archiveprefix = {arxiv},
  keywords = {notion},
  file = {G\:\\내 드라이브\\Papers\\An image is worth 16x16 words - Transformers for image recognition at scale_2020_Dosovitskiy et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\39NM52I3\\2010.html}
}

@inproceedings{Esmaeili2019,
  title = {Structured Disentangled Representations},
  booktitle = {The 22nd {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Esmaeili, Babak and Wu, Hao and Jain, Sarthak and Bozkurt, Alican and Siddharth, Narayanaswamy and Paige, Brooks and Brooks, Dana H. and Dy, Jennifer and Meent, Jan-Willem},
  year = {2019},
  pages = {2525--2534},
  publisher = {{PMLR}},
  keywords = {notion},
  file = {G\:\\내 드라이브\\Papers\\Structured disentangled representations_2019_Esmaeili et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\PGPBPACB\\esmaeili19a.html}
}

@misc{Esser2020,
  title = {Learned {{Step Size Quantization}}},
  author = {Esser, Steven K. and McKinstry, Jeffrey L. and Bablani, Deepika and Appuswamy, Rathinakumar and Modha, Dharmendra S.},
  year = {2020},
  month = may,
  number = {arXiv:1902.08153},
  eprint = {1902.08153},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1902.08153},
  urldate = {2023-09-18},
  abstract = {Deep networks run with low precision operations at inference time offer power and space advantages over high precision alternatives, but need to overcome the challenge of maintaining high accuracy as precision decreases. Here, we present a method for training such networks, Learned Step Size Quantization, that achieves the highest accuracy to date on the ImageNet dataset when using models, from a variety of architectures, with weights and activations quantized to 2-, 3- or 4-bits of precision, and that can train 3-bit models that reach full precision baseline accuracy. Our approach builds upon existing methods for learning weights in quantized networks by improving how the quantizer itself is configured. Specifically, we introduce a novel means to estimate and scale the task loss gradient at each weight and activation layer's quantizer step size, such that it can be learned in conjunction with other network parameters. This approach works using different levels of precision as needed for a given system and requires only a simple modification of existing training code.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {G\:\\내 드라이브\\Papers\\Learned Step Size Quantization_2020_Esser et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\9JX24J8L\\1902.html}
}

@incollection{Feller2015,
  title = {On the Theory of Stochastic Processes, with Particular Reference to Applications},
  booktitle = {Selected {{Papers I}}},
  author = {Feller, William},
  year = {2015},
  pages = {769--798},
  publisher = {{Springer}},
  keywords = {notion},
  file = {C:\Users\giwon\Zotero\storage\ZZBTAA4J\978-3-319-16859-3_42.html}
}

@inproceedings{Gallo2013,
  title = {{{GeRNet}}: {{A Framework}} for {{Inference}}, {{Visualization}} and {{Manipulation}} of {{Gene Regulatory Networks}} Based on {{Association Rules}}},
  shorttitle = {{{GeRNet}}},
  booktitle = {{{VI Congreso Argentino}} de {{Bioinform{\'a}tica}} y {{Biolog{\'i}a Computacional}}, 29-31 de Octubre de 2013, {{Rosario}}, {{Argentina}}},
  author = {Gallo, Cristian A. and Carballido, Jessica A. and Ponzoni, Ignacio},
  year = {2013},
  keywords = {notion},
  file = {G:\내 드라이브\Papers\GeRNet - A Framework for Inference, Visualization and Manipulation of Gene_2013_Gallo et al.pdf}
}

@misc{Gandhi2023,
  title = {Distil-{{Whisper}}: {{Robust Knowledge Distillation}} via {{Large-Scale Pseudo Labelling}}},
  shorttitle = {Distil-{{Whisper}}},
  author = {Gandhi, Sanchit and {von Platen}, Patrick and Rush, Alexander M.},
  year = {2023},
  month = nov,
  number = {arXiv:2311.00430},
  eprint = {2311.00430},
  primaryclass = {cs, eess},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2311.00430},
  urldate = {2023-12-15},
  abstract = {As the size of pre-trained speech recognition models increases, running these large models in low-latency or resource-constrained environments becomes challenging. In this work, we leverage pseudo-labelling to assemble a large-scale open-source dataset which we use to distill the Whisper model into a smaller variant, called Distil-Whisper. Using a simple word error rate (WER) heuristic, we select only the highest quality pseudo-labels for training. The distilled model is 5.8 times faster with 51\% fewer parameters, while performing to within 1\% WER on out-of-distribution test data in a zero-shot transfer setting. Distil-Whisper maintains the robustness of the Whisper model to difficult acoustic conditions, while being less prone to hallucination errors on long-form audio. Distil-Whisper is designed to be paired with Whisper for speculative decoding, yielding a 2 times speed-up while mathematically ensuring the same outputs as the original model. To facilitate further research in this domain, we make our training code, inference code and models publicly accessible.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {G\:\\내 드라이브\\Papers\\Distil-Whisper - Robust Knowledge Distillation via Large-Scale Pseudo Labelling_2023_Gandhi et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\FLB28RK9\\2311.html}
}

@inproceedings{Gatys2016,
  title = {Image Style Transfer Using Convolutional Neural Networks},
  booktitle = {Proceedings of the {{IEEE}} Conference on Computer Vision and Pattern Recognition},
  author = {Gatys, Leon A. and Ecker, Alexander S. and Bethge, Matthias},
  year = {2016},
  pages = {2414--2423},
  keywords = {⛔ No DOI found,notion},
  file = {G\:\\내 드라이브\\Papers\\Image style transfer using convolutional neural networks_2016_Gatys et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\2GYYJEA4\\Gatys_Image_Style_Transfer_CVPR_2016_paper.html}
}

@article{Geiping2022,
  title = {Cramming: {{Training}} a {{Language Model}} on a {{Single GPU}} in {{One Day}}},
  shorttitle = {Cramming},
  author = {Geiping, Jonas and Goldstein, Tom},
  year = {2022},
  journal = {arXiv preprint arXiv:2212.14034},
  eprint = {2212.14034},
  archiveprefix = {arxiv},
  keywords = {notion},
  file = {G\:\\내 드라이브\\Papers\\Cramming - Training a Language Model on a Single GPU in One Day_2022_Geiping et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\42NPVFAG\\2212.html}
}

@article{Ghiasi2022,
  title = {What Do {{Vision Transformers Learn}}? {{A Visual Exploration}}},
  shorttitle = {What Do {{Vision Transformers Learn}}?},
  author = {Ghiasi, Amin and Kazemi, Hamid and Borgnia, Eitan and Reich, Steven and Shu, Manli and Goldblum, Micah and Wilson, Andrew Gordon and Goldstein, Tom},
  year = {2022},
  journal = {arXiv preprint arXiv:2212.06727},
  eprint = {2212.06727},
  archiveprefix = {arxiv},
  keywords = {notion},
  file = {G\:\\내 드라이브\\Papers\\What do Vision Transformers Learn - A Visual Exploration_2022_Ghiasi et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\35RBM4U9\\2212.html}
}

@inproceedings{Girshick2014,
  title = {Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation},
  booktitle = {Proceedings of the {{IEEE}} Conference on Computer Vision and Pattern Recognition},
  author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
  year = {2014},
  pages = {580--587},
  keywords = {notion},
  file = {G\:\\내 드라이브\\Papers\\Rich feature hierarchies for accurate object detection and semantic segmentation_2014_Girshick et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\7ISA8EJU\\Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.html}
}

@inproceedings{Girshick2015,
  title = {Fast R-Cnn},
  booktitle = {Proceedings of the {{IEEE}} International Conference on Computer Vision},
  author = {Girshick, Ross},
  year = {2015},
  pages = {1440--1448},
  keywords = {notion},
  file = {G\:\\내 드라이브\\Papers\\Fast r-cnn_2015_Girshick.pdf;C\:\\Users\\giwon\\Zotero\\storage\\VC37XP9H\\Girshick_Fast_R-CNN_ICCV_2015_paper.html}
}

@misc{Goodfellow2014,
  title = {Generative {{Adversarial Networks}}},
  author = {Goodfellow, Ian J. and {Pouget-Abadie}, Jean and Mirza, Mehdi and Xu, Bing and {Warde-Farley}, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  year = {2014},
  month = jun,
  number = {arXiv:1406.2661},
  eprint = {1406.2661},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1406.2661},
  urldate = {2023-05-24},
  abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,notion,Statistics - Machine Learning},
  file = {C\:\\Users\\giwon\\Zotero\\storage\\IIXRRVVW\\Goodfellow et al. - 2014 - Generative Adversarial Networks.pdf;C\:\\Users\\giwon\\Zotero\\storage\\DN64Y35A\\1406.html}
}

@inproceedings{Guler2018,
  title = {Densepose: {{Dense}} Human Pose Estimation in the Wild},
  shorttitle = {Densepose},
  booktitle = {Proceedings of the {{IEEE}} Conference on Computer Vision and Pattern Recognition},
  author = {G{\"u}ler, R{\i}za Alp and Neverova, Natalia and Kokkinos, Iasonas},
  year = {2018},
  pages = {7297--7306},
  keywords = {notion},
  file = {G\:\\내 드라이브\\Papers\\Densepose - Dense human pose estimation in the wild_2018_Güler et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\WKKWH434\\Guler_DensePose_Dense_Human_CVPR_2018_paper.html}
}

@misc{He2015,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year = {2015},
  month = dec,
  number = {arXiv:1512.03385},
  eprint = {1512.03385},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1512.03385},
  urldate = {2023-03-14},
  abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,notion},
  file = {G\:\\내 드라이브\\Papers\\Deep Residual Learning for Image Recognition_2015_He et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\A63UQNJT\\1512.html}
}

@inproceedings{He2017,
  title = {Mask R-Cnn},
  booktitle = {Proceedings of the {{IEEE}} International Conference on Computer Vision},
  author = {He, Kaiming and Gkioxari, Georgia and Doll{\'a}r, Piotr and Girshick, Ross},
  year = {2017},
  pages = {2961--2969},
  keywords = {notion},
  file = {G\:\\내 드라이브\\Papers\\Mask r-cnn_2017_He et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\MUUSLE3Q\\He_Mask_R-CNN_ICCV_2017_paper.html}
}

@misc{Hendrycks2018,
  title = {A {{Baseline}} for {{Detecting Misclassified}} and {{Out-of-Distribution Examples}} in {{Neural Networks}}},
  author = {Hendrycks, Dan and Gimpel, Kevin},
  year = {2018},
  month = oct,
  number = {arXiv:1610.02136},
  eprint = {1610.02136},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1610.02136},
  urldate = {2023-06-02},
  abstract = {We consider the two related problems of detecting if an example is misclassified or out-of-distribution. We present a simple baseline that utilizes probabilities from softmax distributions. Correctly classified examples tend to have greater maximum softmax probabilities than erroneously classified and out-of-distribution examples, allowing for their detection. We assess performance by defining several tasks in computer vision, natural language processing, and automatic speech recognition, showing the effectiveness of this baseline across all. We then show the baseline can sometimes be surpassed, demonstrating the room for future research on these underexplored detection tasks.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,notion},
  file = {G\:\\내 드라이브\\Papers\\A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks_2018_Hendrycks et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\I3YEHGFX\\1610.html}
}

@misc{Hendrycks2019,
  title = {Deep {{Anomaly Detection}} with {{Outlier Exposure}}},
  author = {Hendrycks, Dan and Mazeika, Mantas and Dietterich, Thomas},
  year = {2019},
  month = jan,
  number = {arXiv:1812.04606},
  eprint = {1812.04606},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1812.04606},
  urldate = {2023-06-02},
  abstract = {It is important to detect anomalous inputs when deploying machine learning systems. The use of larger and more complex inputs in deep learning magnifies the difficulty of distinguishing between anomalous and in-distribution examples. At the same time, diverse image and text data are available in enormous quantities. We propose leveraging these data to improve deep anomaly detection by training anomaly detectors against an auxiliary dataset of outliers, an approach we call Outlier Exposure (OE). This enables anomaly detectors to generalize and detect unseen anomalies. In extensive experiments on natural language processing and small- and large-scale vision tasks, we find that Outlier Exposure significantly improves detection performance. We also observe that cutting-edge generative models trained on CIFAR-10 may assign higher likelihoods to SVHN images than to CIFAR-10 images; we use OE to mitigate this issue. We also analyze the flexibility and robustness of Outlier Exposure, and identify characteristics of the auxiliary dataset that improve performance.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,notion,Statistics - Machine Learning},
  file = {G\:\\내 드라이브\\Papers\\Deep Anomaly Detection with Outlier Exposure_2019_Hendrycks et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\LT2XERF8\\1812.html}
}

@article{Higgins2016,
  title = {Beta-Vae: {{Learning}} Basic Visual Concepts with a Constrained Variational Framework},
  shorttitle = {Beta-Vae},
  author = {Higgins, Irina and Matthey, Loic and Pal, Arka and Burgess, Christopher and Glorot, Xavier and Botvinick, Matthew and Mohamed, Shakir and Lerchner, Alexander},
  year = {2016},
  keywords = {notion},
  file = {C:\Users\giwon\Zotero\storage\7XANZXTF\forum.html}
}

@article{Hinton2012,
  title = {Improving Neural Networks by Preventing Co-Adaptation of Feature Detectors},
  author = {Hinton, Geoffrey E. and Srivastava, Nitish and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan R.},
  year = {2012},
  journal = {arXiv preprint arXiv:1207.0580},
  eprint = {1207.0580},
  archiveprefix = {arxiv},
  keywords = {notion},
  file = {G\:\\내 드라이브\\Papers\\Improving neural networks by preventing co-adaptation of feature detectors_2012_Hinton et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\Z9GH65SN\\1207.html}
}

@article{Ho2020,
  title = {Denoising Diffusion Probabilistic Models},
  author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  year = {2020},
  journal = {Advances in Neural Information Processing Systems},
  volume = {33},
  pages = {6840--6851},
  keywords = {notion},
  file = {G\:\\내 드라이브\\Papers\\Denoising diffusion probabilistic models_2020_Ho et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\LNJV8L9J\\4c5bcfec8584af0d967f1ab10179ca4b-Abstract.html}
}

@article{Ho2022,
  title = {Classifier-Free Diffusion Guidance},
  author = {Ho, Jonathan and Salimans, Tim},
  year = {2022},
  journal = {arXiv preprint arXiv:2207.12598},
  eprint = {2207.12598},
  archiveprefix = {arxiv},
  keywords = {notion},
  file = {G\:\\내 드라이브\\Papers\\Classifier-free diffusion guidance_2022_Ho et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\RXAWVKPN\\2207.html}
}

@inproceedings{Hong2021,
  title = {Generalization of {{Construction Object Segmentation Models}} Using {{Self-Supervised Learning}}},
  booktitle = {{{ISARC}}. {{Proceedings}} of the {{International Symposium}} on {{Automation}} and {{Robotics}} in {{Construction}}},
  author = {Hong, Yeji and Chern, Wei Chih and Nguyen, Tam and Kim, Hongjo},
  year = {2021},
  volume = {38},
  pages = {843--848},
  publisher = {{IAARC Publications}},
  keywords = {notion},
  file = {G\:\\내 드라이브\\Papers\\Generalization of Construction Object Segmentation Models using Self-Supervised_2021_Hong et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\K2UHFV85\\intermediateredirectforezproxy.html}
}

@misc{Hu2021,
  title = {{{LoRA}}: {{Low-Rank Adaptation}} of {{Large Language Models}}},
  shorttitle = {{{LoRA}}},
  author = {Hu, Edward J. and Shen, Yelong and Wallis, Phillip and {Allen-Zhu}, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  year = {2021},
  month = oct,
  number = {arXiv:2106.09685},
  eprint = {2106.09685},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2106.09685},
  urldate = {2023-02-09},
  abstract = {An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,notion},
  file = {G\:\\내 드라이브\\Papers\\LoRA - Low-Rank Adaptation of Large Language Models_2021_Hu et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\RGLK97RV\\2106.html}
}

@article{Hubara2016,
  title = {Binarized Neural Networks},
  author = {Hubara, Itay and Courbariaux, Matthieu and Soudry, Daniel and {El-Yaniv}, Ran and Bengio, Yoshua},
  year = {2016},
  journal = {Advances in neural information processing systems},
  volume = {29},
  keywords = {⛔ No DOI found},
  file = {G\:\\내 드라이브\\Papers\\Binarized neural networks_2016_Hubara et al2.pdf;C\:\\Users\\giwon\\Zotero\\storage\\P2T43VAW\\d8330f857a17c53d217014ee776bfd50-Abstract.html}
}

@inproceedings{Hubara2016a,
  title = {Binarized {{Neural Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Hubara, Itay and Courbariaux, Matthieu and Soudry, Daniel and {El-Yaniv}, Ran and Bengio, Yoshua},
  year = {2016},
  volume = {29},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2023-09-04},
  abstract = {We introduce a method to train Binarized Neural Networks (BNNs) - neural networks with binary weights and activations at run-time. At train-time the binary weights and activations are used for computing the parameter gradients. During the forward pass, BNNs drastically reduce memory size and accesses, and replace most arithmetic operations with bit-wise operations, which is expected to  substantially improve power-efficiency. To validate the effectiveness of BNNs, we conducted two sets of experiments on the Torch7 and Theano frameworks. On both, BNNs achieved nearly state-of-the-art results over the MNIST, CIFAR-10 and SVHN datasets. We also report our preliminary results on the challenging ImageNet dataset. Last but not least, we wrote a binary matrix multiplication GPU kernel with which it is possible to run our MNIST BNN 7 times faster  than with an unoptimized GPU kernel, without suffering any loss in classification accuracy. The code for training and running our BNNs is available on-line.},
  keywords = {⛔ No DOI found,notion},
  file = {G:\내 드라이브\Papers\Binarized Neural Networks_2016_Hubara et al.pdf}
}

@inproceedings{Ioffe2015,
  title = {Batch Normalization: {{Accelerating}} Deep Network Training by Reducing Internal Covariate Shift},
  shorttitle = {Batch Normalization},
  booktitle = {International Conference on Machine Learning},
  author = {Ioffe, Sergey and Szegedy, Christian},
  year = {2015},
  pages = {448--456},
  publisher = {{PMLR}},
  keywords = {notion},
  file = {G\:\\내 드라이브\\Papers\\Batch normalization - Accelerating deep network training by reducing internal_2015_Ioffe et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\2TLAMETY\\ioffe15.html}
}

@misc{Isola2018,
  title = {Image-to-{{Image Translation}} with {{Conditional Adversarial Networks}}},
  author = {Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A.},
  year = {2018},
  month = nov,
  number = {arXiv:1611.07004},
  eprint = {1611.07004},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1611.07004},
  urldate = {2023-06-01},
  abstract = {We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. Indeed, since the release of the pix2pix software associated with this paper, a large number of internet users (many of them artists) have posted their own experiments with our system, further demonstrating its wide applicability and ease of adoption without the need for parameter tweaking. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without hand-engineering our loss functions either.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,notion},
  file = {G\:\\내 드라이브\\Papers\\Image-to-Image Translation with Conditional Adversarial Networks_2018_Isola et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\3W6N2EU3\\1611.html}
}

@inproceedings{Izadi2011,
  title = {{{KinectFusion}}: Real-Time {{3D}} Reconstruction and Interaction Using a Moving Depth Camera},
  shorttitle = {{{KinectFusion}}},
  booktitle = {Proceedings of the 24th Annual {{ACM}} Symposium on {{User}} Interface Software and Technology},
  author = {Izadi, Shahram and Kim, David and Hilliges, Otmar and Molyneaux, David and Newcombe, Richard and Kohli, Pushmeet and Shotton, Jamie and Hodges, Steve and Freeman, Dustin and Davison, Andrew},
  year = {2011},
  pages = {559--568},
  keywords = {notion},
  file = {G\:\\내 드라이브\\Papers\\KinectFusion - real-time 3D reconstruction and interaction using a moving depth_2011_Izadi et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\U8V29MG8\\2047196.html}
}

@misc{Jaderberg2016,
  title = {Spatial {{Transformer Networks}}},
  author = {Jaderberg, Max and Simonyan, Karen and Zisserman, Andrew and Kavukcuoglu, Koray},
  year = {2016},
  month = feb,
  number = {arXiv:1506.02025},
  eprint = {1506.02025},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1506.02025},
  urldate = {2023-03-21},
  abstract = {Convolutional Neural Networks define an exceptionally powerful class of models, but are still limited by the lack of ability to be spatially invariant to the input data in a computationally and parameter efficient manner. In this work we introduce a new learnable module, the Spatial Transformer, which explicitly allows the spatial manipulation of data within the network. This differentiable module can be inserted into existing convolutional architectures, giving neural networks the ability to actively spatially transform feature maps, conditional on the feature map itself, without any extra training supervision or modification to the optimisation process. We show that the use of spatial transformers results in models which learn invariance to translation, scale, rotation and more generic warping, resulting in state-of-the-art performance on several benchmarks, and for a number of classes of transformations.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,notion},
  file = {G\:\\내 드라이브\\Papers\\Spatial Transformer Networks_2016_Jaderberg et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\BSVWDMWH\\1506.html}
}

@misc{Jiang2021,
  title = {Learning to {{Estimate Hidden Motions}} with {{Global Motion Aggregation}}},
  author = {Jiang, Shihao and Campbell, Dylan and Lu, Yao and Li, Hongdong and Hartley, Richard},
  year = {2021},
  month = jul,
  number = {arXiv:2104.02409},
  eprint = {2104.02409},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2104.02409},
  urldate = {2023-03-13},
  abstract = {Occlusions pose a significant challenge to optical flow algorithms that rely on local evidences. We consider an occluded point to be one that is imaged in the first frame but not in the next, a slight overloading of the standard definition since it also includes points that move out-of-frame. Estimating the motion of these points is extremely difficult, particularly in the two-frame setting. Previous work relies on CNNs to learn occlusions, without much success, or requires multiple frames to reason about occlusions using temporal smoothness. In this paper, we argue that the occlusion problem can be better solved in the two-frame case by modelling image self-similarities. We introduce a global motion aggregation module, a transformer-based approach to find long-range dependencies between pixels in the first image, and perform global aggregation on the corresponding motion features. We demonstrate that the optical flow estimates in the occluded regions can be significantly improved without damaging the performance in non-occluded regions. This approach obtains new state-of-the-art results on the challenging Sintel dataset, improving the average end-point error by 13.6\% on Sintel Final and 13.7\% on Sintel Clean. At the time of submission, our method ranks first on these benchmarks among all published and unpublished approaches. Code is available at https://github.com/zacjiang/GMA},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,notion},
  file = {G\:\\내 드라이브\\Papers\\Learning to Estimate Hidden Motions with Global Motion Aggregation_2021_Jiang et al2.pdf;C\:\\Users\\giwon\\Zotero\\storage\\YJKJ4INS\\2104.html}
}

@inproceedings{Jiang2021a,
  title = {Learning to Estimate Hidden Motions with Global Motion Aggregation},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}}},
  author = {Jiang, Shihao and Campbell, Dylan and Lu, Yao and Li, Hongdong and Hartley, Richard},
  year = {2021},
  pages = {9772--9781},
  keywords = {⛔ No DOI found,notion},
  file = {G\:\\내 드라이브\\Papers\\Learning to estimate hidden motions with global motion aggregation_2021_Jiang et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\MDGAE773\\Jiang_Learning_To_Estimate_Hidden_Motions_With_Global_Motion_Aggregation_ICCV_2021_paper.html}
}

@inproceedings{Johnson2016,
  title = {Perceptual Losses for Real-Time Style Transfer and Super-Resolution},
  booktitle = {Computer {{Vision}}{\textendash}{{ECCV}} 2016: 14th {{European Conference}}, {{Amsterdam}}, {{The Netherlands}}, {{October}} 11-14, 2016, {{Proceedings}}, {{Part II}} 14},
  author = {Johnson, Justin and Alahi, Alexandre and {Fei-Fei}, Li},
  year = {2016},
  pages = {694--711},
  publisher = {{Springer}},
  keywords = {⛔ No DOI found,notion},
  file = {G\:\\내 드라이브\\Papers\\Perceptual losses for real-time style transfer and super-resolution_2016_Johnson et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\GXMV4AHC\\978-3-319-46475-6_43.html}
}

@article{Jumper2021,
  title = {Highly Accurate Protein Structure Prediction with {{AlphaFold}}},
  author = {Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and {\v Z}{\'i}dek, Augustin and Potapenko, Anna},
  year = {2021},
  journal = {Nature},
  volume = {596},
  number = {7873},
  pages = {583--589},
  publisher = {{Nature Publishing Group}},
  keywords = {notion},
  file = {C:\Users\giwon\Zotero\storage\P9AAKKY9\s41586-021-03819-2.html}
}

@misc{Jung2018,
  title = {Learning to {{Quantize Deep Networks}} by {{Optimizing Quantization Intervals}} with {{Task Loss}}},
  author = {Jung, Sangil and Son, Changyong and Lee, Seohyung and Son, Jinwoo and Kwak, Youngjun and Han, Jae-Joon and Hwang, Sung Ju and Choi, Changkyu},
  year = {2018},
  month = nov,
  number = {arXiv:1808.05779},
  eprint = {1808.05779},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1808.05779},
  urldate = {2023-10-03},
  abstract = {Reducing bit-widths of activations and weights of deep networks makes it efficient to compute and store them in memory, which is crucial in their deployments to resource-limited devices, such as mobile phones. However, decreasing bit-widths with quantization generally yields drastically degraded accuracy. To tackle this problem, we propose to learn to quantize activations and weights via a trainable quantizer that transforms and discretizes them. Specifically, we parameterize the quantization intervals and obtain their optimal values by directly minimizing the task loss of the network. This quantization-interval-learning (QIL) allows the quantized networks to maintain the accuracy of the full-precision (32-bit) networks with bit-width as low as 4-bit and minimize the accuracy degeneration with further bit-width reduction (i.e., 3 and 2-bit). Moreover, our quantizer can be trained on a heterogeneous dataset, and thus can be used to quantize pretrained networks without access to their training data. We demonstrate the effectiveness of our trainable quantizer on ImageNet dataset with various network architectures such as ResNet-18, -34 and AlexNet, on which it outperforms existing methods to achieve the state-of-the-art accuracy.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,notion},
  file = {G\:\\내 드라이브\\Papers\\Learning to Quantize Deep Networks by Optimizing Quantization Intervals with Task Loss_2018_Jung et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\MZDWIEPN\\1808.html}
}

@misc{Jung2021,
  title = {Spell My Name: Keyword Boosted Speech Recognition},
  shorttitle = {Spell My Name},
  author = {Jung, Namkyu and Kim, Geonmin and Chung, Joon Son},
  year = {2021},
  month = oct,
  number = {arXiv:2110.02791},
  eprint = {2110.02791},
  primaryclass = {cs, eess},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2110.02791},
  urldate = {2023-12-16},
  abstract = {Recognition of uncommon words such as names and technical terminology is important to understanding conversations in context. However, the ability to recognise such words remains a challenge in modern automatic speech recognition (ASR) systems. In this paper, we propose a simple but powerful ASR decoding method that can better recognise these uncommon keywords, which in turn enables better readability of the results. The method boosts the probabilities of given keywords in a beam search based on acoustic model predictions. The method does not require any training in advance. We demonstrate the effectiveness of our method on the LibriSpeeech test sets and also internal data of real-world conversations. Our method significantly boosts keyword accuracy on the test sets, while maintaining the accuracy of the other words, and as well as providing significant qualitative improvements. This method is applicable to other tasks such as machine translation, or wherever unseen and difficult keywords need to be recognised in beam search.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {G\:\\내 드라이브\\Papers\\Spell my name - keyword boosted speech recognition_2021_Jung et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\38LMBF34\\2110.html}
}

@article{Karras2022,
  title = {Elucidating the {{Design Space}} of {{Diffusion-Based Generative Models}}},
  author = {Karras, Tero and Aittala, Miika and Aila, Timo and Laine, Samuli},
  year = {2022},
  journal = {arXiv preprint arXiv:2206.00364},
  eprint = {2206.00364},
  archiveprefix = {arxiv},
  keywords = {notion},
  file = {G\:\\내 드라이브\\Papers\\Elucidating the Design Space of Diffusion-Based Generative Models_2022_Karras et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\JCK3PXKU\\2206.html}
}

@inproceedings{Khoreva2017,
  title = {Simple Does It: {{Weakly}} Supervised Instance and Semantic Segmentation},
  shorttitle = {Simple Does It},
  booktitle = {Proceedings of the {{IEEE}} Conference on Computer Vision and Pattern Recognition},
  author = {Khoreva, Anna and Benenson, Rodrigo and Hosang, Jan and Hein, Matthias and Schiele, Bernt},
  year = {2017},
  pages = {876--885},
  keywords = {notion},
  file = {G\:\\내 드라이브\\Papers\\Simple does it - Weakly supervised instance and semantic segmentation_2017_Khoreva et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\T43ZZXJC\\Khoreva_Simple_Does_It_CVPR_2017_paper.html}
}

@inproceedings{Kim2018,
  title = {Disentangling by Factorising},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Kim, Hyunjik and Mnih, Andriy},
  year = {2018},
  pages = {2649--2658},
  publisher = {{PMLR}},
  keywords = {notion},
  file = {G\:\\내 드라이브\\Papers\\Disentangling by factorising_2018_Kim et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\ED5QYRBI\\kim18b.html}
}

@article{Kingma2013,
  title = {Auto-Encoding Variational Bayes},
  author = {Kingma, Diederik P. and Welling, Max},
  year = {2013},
  journal = {arXiv preprint arXiv:1312.6114},
  eprint = {1312.6114},
  archiveprefix = {arxiv},
  keywords = {notion},
  file = {G\:\\내 드라이브\\Papers\\Auto-encoding variational bayes_2013_Kingma et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\W5TM9FBA\\1312.html}
}

@article{Kingma2014,
  title = {Adam: {{A}} Method for Stochastic Optimization},
  shorttitle = {Adam},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  year = {2014},
  journal = {arXiv preprint arXiv:1412.6980},
  eprint = {1412.6980},
  archiveprefix = {arxiv},
  keywords = {notion},
  file = {G\:\\내 드라이브\\Papers\\Adam - A method for stochastic optimization_2014_Kingma et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\RKBFR2L5\\1412.html}
}

@article{Kingma2021,
  title = {Variational Diffusion Models},
  author = {Kingma, Diederik and Salimans, Tim and Poole, Ben and Ho, Jonathan},
  year = {2021},
  journal = {Advances in neural information processing systems},
  volume = {34},
  pages = {21696--21707},
  keywords = {notion},
  file = {G\:\\내 드라이브\\Papers\\Variational diffusion models_2021_Kingma et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\V3YUS8S9\\b578f2a52a0229873fefc2a4b06377fa-Abstract.html}
}

@article{Kisantal2019,
  title = {Augmentation for Small Object Detection},
  author = {Kisantal, Mate and Wojna, Zbigniew and Murawski, Jakub and Naruniec, Jacek and Cho, Kyunghyun},
  year = {2019},
  journal = {arXiv preprint arXiv:1902.07296},
  eprint = {1902.07296},
  archiveprefix = {arxiv},
  keywords = {notion},
  file = {G\:\\내 드라이브\\Papers\\Augmentation for small object detection_2019_Kisantal et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\9AYQVB9W\\1902.html}
}

@article{Lafferty2001,
  title = {Conditional Random Fields: {{Probabilistic}} Models for Segmenting and Labeling Sequence Data},
  shorttitle = {Conditional Random Fields},
  author = {Lafferty, John and McCallum, Andrew and Pereira, Fernando CN},
  year = {2001},
  keywords = {notion},
  file = {G\:\\내 드라이브\\Papers\\Conditional random fields - Probabilistic models for segmenting and labeling_2001_Lafferty et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\KYAXT8S3\\159.html}
}

@article{Lan2023,
  title = {Vision {{Transformers Are Good Mask Auto-Labelers}}},
  author = {Lan, Shiyi and Yang, Xitong and Yu, Zhiding and Wu, Zuxuan and Alvarez, Jose M. and Anandkumar, Anima},
  year = {2023},
  journal = {arXiv preprint arXiv:2301.03992},
  eprint = {2301.03992},
  archiveprefix = {arxiv},
  keywords = {notion},
  file = {G\:\\내 드라이브\\Papers\\Vision Transformers Are Good Mask Auto-Labelers_2023_Lan et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\PH7R6HJQ\\2301.html}
}

@misc{Lee2018,
  title = {A {{Simple Unified Framework}} for {{Detecting Out-of-Distribution Samples}} and {{Adversarial Attacks}}},
  author = {Lee, Kimin and Lee, Kibok and Lee, Honglak and Shin, Jinwoo},
  year = {2018},
  month = oct,
  number = {arXiv:1807.03888},
  eprint = {1807.03888},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1807.03888},
  urldate = {2023-06-02},
  abstract = {Detecting test samples drawn sufficiently far away from the training distribution statistically or adversarially is a fundamental requirement for deploying a good classifier in many real-world machine learning applications. However, deep neural networks with the softmax classifier are known to produce highly overconfident posterior distributions even for such abnormal samples. In this paper, we propose a simple yet effective method for detecting any abnormal samples, which is applicable to any pre-trained softmax neural classifier. We obtain the class conditional Gaussian distributions with respect to (low- and upper-level) features of the deep models under Gaussian discriminant analysis, which result in a confidence score based on the Mahalanobis distance. While most prior methods have been evaluated for detecting either out-of-distribution or adversarial samples, but not both, the proposed method achieves the state-of-the-art performances for both cases in our experiments. Moreover, we found that our proposed method is more robust in harsh cases, e.g., when the training dataset has noisy labels or small number of samples. Finally, we show that the proposed method enjoys broader usage by applying it to class-incremental learning: whenever out-of-distribution samples are detected, our classification rule can incorporate new classes well without further training deep models.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning,notion,Statistics - Machine Learning},
  file = {G\:\\내 드라이브\\Papers\\A Simple Unified Framework for Detecting Out-of-Distribution Samples and Adversarial Attacks_2018_Lee et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\EQTWHNXL\\1807.html}
}

@misc{Lee2018a,
  title = {Training {{Confidence-calibrated Classifiers}} for {{Detecting Out-of-Distribution Samples}}},
  author = {Lee, Kimin and Lee, Honglak and Lee, Kibok and Shin, Jinwoo},
  year = {2018},
  month = feb,
  number = {arXiv:1711.09325},
  eprint = {1711.09325},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1711.09325},
  urldate = {2023-06-02},
  abstract = {The problem of detecting whether a test sample is from in-distribution (i.e., training distribution by a classifier) or out-of-distribution sufficiently different from it arises in many real-world machine learning applications. However, the state-of-art deep neural networks are known to be highly overconfident in their predictions, i.e., do not distinguish in- and out-of-distributions. Recently, to handle this issue, several threshold-based detectors have been proposed given pre-trained neural classifiers. However, the performance of prior works highly depends on how to train the classifiers since they only focus on improving inference procedures. In this paper, we develop a novel training method for classifiers so that such inference algorithms can work better. In particular, we suggest two additional terms added to the original loss (e.g., cross entropy). The first one forces samples from out-of-distribution less confident by the classifier and the second one is for (implicitly) generating most effective training samples for the first one. In essence, our method jointly trains both classification and generative neural networks for out-of-distribution. We demonstrate its effectiveness using deep convolutional neural networks on various popular image datasets.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,notion,Statistics - Machine Learning},
  file = {G\:\\내 드라이브\\Papers\\Training Confidence-calibrated Classifiers for Detecting Out-of-Distribution Samples_2018_Lee et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\XJYDI9GC\\1711.html}
}

@misc{Lee2020,
  title = {Learning with {{Privileged Information}} for {{Efficient Image Super-Resolution}}},
  author = {Lee, Wonkyung and Lee, Junghyup and Kim, Dohyung and Ham, Bumsub},
  year = {2020},
  month = jul,
  number = {arXiv:2007.07524},
  eprint = {2007.07524},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2007.07524},
  urldate = {2023-09-08},
  abstract = {Convolutional neural networks (CNNs) have allowed remarkable advances in single image super-resolution (SISR) over the last decade. Most SR methods based on CNNs have focused on achieving performance gains in terms of quality metrics, such as PSNR and SSIM, over classical approaches. They typically require a large amount of memory and computational units. FSRCNN, consisting of few numbers of convolutional layers, has shown promising results, while using an extremely small number of network parameters. We introduce in this paper a novel distillation framework, consisting of teacher and student networks, that allows to boost the performance of FSRCNN drastically. To this end, we propose to use ground-truth high-resolution (HR) images as privileged information. The encoder in the teacher learns the degradation process, subsampling of HR images, using an imitation loss. The student and the decoder in the teacher, having the same network architecture as FSRCNN, try to reconstruct HR images. Intermediate features in the decoder, affordable for the student to learn, are transferred to the student through feature distillation. Experimental results on standard benchmarks demonstrate the effectiveness and the generalization ability of our framework, which significantly boosts the performance of FSRCNN as well as other SR methods. Our code and model are available online: https://cvlab.yonsei.ac.kr/projects/PISR.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,notion},
  file = {G\:\\내 드라이브\\Papers\\Learning with Privileged Information for Efficient Image Super-Resolution_2020_Lee et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\FQBC9UJI\\2007.html}
}

@misc{Lee2023,
  title = {Camera-{{Driven Representation Learning}} for {{Unsupervised Domain Adaptive Person Re-identification}}},
  author = {Lee, Geon and Lee, Sanghoon and Kim, Dohyung and Shin, Younghoon and Yoon, Yongsang and Ham, Bumsub},
  year = {2023},
  month = aug,
  number = {arXiv:2308.11901},
  eprint = {2308.11901},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2308.11901},
  urldate = {2023-09-03},
  abstract = {We present a novel unsupervised domain adaption method for person re-identification (reID) that generalizes a model trained on a labeled source domain to an unlabeled target domain. We introduce a camera-driven curriculum learning (CaCL) framework that leverages camera labels of person images to transfer knowledge from source to target domains progressively. To this end, we divide target domain dataset into multiple subsets based on the camera labels, and initially train our model with a single subset (i.e., images captured by a single camera). We then gradually exploit more subsets for training, according to a curriculum sequence obtained with a camera-driven scheduling rule. The scheduler considers maximum mean discrepancies (MMD) between each subset and the source domain dataset, such that the subset closer to the source domain is exploited earlier within the curriculum. For each curriculum sequence, we generate pseudo labels of person images in a target domain to train a reID model in a supervised way. We have observed that the pseudo labels are highly biased toward cameras, suggesting that person images obtained from the same camera are likely to have the same pseudo labels, even for different IDs. To address the camera bias problem, we also introduce a camera-diversity (CD) loss encouraging person images of the same pseudo label, but captured across various cameras, to involve more for discriminative feature learning, providing person representations robust to inter-camera variations. Experimental results on standard benchmarks, including real-to-real and synthetic-to-real scenarios, demonstrate the effectiveness of our framework.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {G\:\\내 드라이브\\Papers\\Camera-Driven Representation Learning for Unsupervised Domain Adaptive Person Re-identification_2023_Lee et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\MZMWPNQV\\2308.html}
}

@misc{Li2020,
  title = {Additive {{Powers-of-Two Quantization}}: {{An Efficient Non-uniform Discretization}} for {{Neural Networks}}},
  shorttitle = {Additive {{Powers-of-Two Quantization}}},
  author = {Li, Yuhang and Dong, Xin and Wang, Wei},
  year = {2020},
  month = feb,
  number = {arXiv:1909.13144},
  eprint = {1909.13144},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1909.13144},
  urldate = {2023-10-03},
  abstract = {We propose Additive Powers-of-Two{\textasciitilde}(APoT) quantization, an efficient non-uniform quantization scheme for the bell-shaped and long-tailed distribution of weights and activations in neural networks. By constraining all quantization levels as the sum of Powers-of-Two terms, APoT quantization enjoys high computational efficiency and a good match with the distribution of weights. A simple reparameterization of the clipping function is applied to generate a better-defined gradient for learning the clipping threshold. Moreover, weight normalization is presented to refine the distribution of weights to make the training more stable and consistent. Experimental results show that our proposed method outperforms state-of-the-art methods, and is even competitive with the full-precision models, demonstrating the effectiveness of our proposed APoT quantization. For example, our 4-bit quantized ResNet-50 on ImageNet achieves 76.6\% top-1 accuracy without bells and whistles; meanwhile, our model reduces 22\% computational cost compared with the uniformly quantized counterpart. The code is available at https://github.com/yhhhli/APoT\_Quantization.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,notion,Statistics - Machine Learning},
  file = {G\:\\내 드라이브\\Papers\\Additive Powers-of-Two Quantization - An Efficient Non-uniform Discretization for Neural Networks_2020_Li et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\PH4I7LBK\\1909.html}
}

@misc{Li2021,
  title = {{{SRDiff}}: {{Single Image Super-Resolution}} with {{Diffusion Probabilistic Models}}},
  shorttitle = {{{SRDiff}}},
  author = {Li, Haoying and Yang, Yifan and Chang, Meng and Feng, Huajun and Xu, Zhihai and Li, Qi and Chen, Yueting},
  year = {2021},
  month = may,
  number = {arXiv:2104.14951},
  eprint = {2104.14951},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2104.14951},
  urldate = {2023-02-02},
  abstract = {Single image super-resolution (SISR) aims to reconstruct high-resolution (HR) images from the given low-resolution (LR) ones, which is an ill-posed problem because one LR image corresponds to multiple HR images. Recently, learning-based SISR methods have greatly outperformed traditional ones, while suffering from over-smoothing, mode collapse or large model footprint issues for PSNR-oriented, GAN-driven and flow-based methods respectively. To solve these problems, we propose a novel single image super-resolution diffusion probabilistic model (SRDiff), which is the first diffusion-based model for SISR. SRDiff is optimized with a variant of the variational bound on the data likelihood and can provide diverse and realistic SR predictions by gradually transforming the Gaussian noise into a super-resolution (SR) image conditioned on an LR input through a Markov chain. In addition, we introduce residual prediction to the whole framework to speed up convergence. Our extensive experiments on facial and general benchmarks (CelebA and DIV2K datasets) show that 1) SRDiff can generate diverse SR results in rich details with state-of-the-art performance, given only one LR input; 2) SRDiff is easy to train with a small footprint; and 3) SRDiff can perform flexible image manipulation including latent space interpolation and content fusion.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,notion},
  file = {G\:\\내 드라이브\\Papers\\SRDiff - Single Image Super-Resolution with Diffusion Probabilistic Models_2021_Li et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\MWP2UGM4\\2104.html}
}

@misc{Li2021a,
  title = {From {{Beginner}} to {{Master}}: {{A Survey}} for {{Deep Learning-based Single-Image Super-Resolution}}},
  shorttitle = {From {{Beginner}} to {{Master}}},
  author = {Li, Juncheng and Pei, Zehua and Zeng, Tieyong},
  year = {2021},
  month = sep,
  number = {arXiv:2109.14335},
  eprint = {2109.14335},
  primaryclass = {cs, eess},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2109.14335},
  urldate = {2023-01-30},
  abstract = {Single-image super-resolution (SISR) is an important task in image processing, which aims to enhance the resolution of imaging systems. Recently, SISR has made a huge leap and has achieved promising results with the help of deep learning (DL). In this survey, we give an overview of DL-based SISR methods and group them according to their targets, such as reconstruction efficiency, reconstruction accuracy, and perceptual accuracy. Specifically, we first introduce the problem definition, research background, and the significance of SISR. Secondly, we introduce some related works, including benchmark datasets, upsampling methods, optimization objectives, and image quality assessment methods. Thirdly, we provide a detailed investigation of SISR and give some domain-specific applications of it. Fourthly, we present the reconstruction results of some classic SISR methods to intuitively know their performance. Finally, we discuss some issues that still exist in SISR and summarize some new trends and future directions. This is an exhaustive survey of SISR, which can help researchers better understand SISR and inspire more exciting research in this field. An investigation project for SISR is provided in https://github.com/CV-JunchengLi/SISR-Survey.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Electrical Engineering and Systems Science - Image and Video Processing,notion},
  file = {G\:\\내 드라이브\\Papers\\From Beginner to Master - A Survey for Deep Learning-based Single-Image_2021_Li et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\A236GDXU\\2109.html}
}

@inproceedings{Li2021b,
  title = {Semantic Segmentation with Generative Models: {{Semi-supervised}} Learning and Strong out-of-Domain Generalization},
  shorttitle = {Semantic Segmentation with Generative Models},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Li, Daiqing and Yang, Junlin and Kreis, Karsten and Torralba, Antonio and Fidler, Sanja},
  year = {2021},
  pages = {8300--8311},
  keywords = {notion},
  file = {G\:\\내 드라이브\\Papers\\Semantic segmentation with generative models - Semi-supervised learning and_2021_Li et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\SCJYF7X6\\Li_Semantic_Segmentation_With_Generative_Models_Semi-Supervised_Learning_and_Strong_Out-of-Doma.html}
}

@misc{Li2022,
  title = {On {{Efficient Transformer-Based Image Pre-training}} for {{Low-Level Vision}}},
  author = {Li, Wenbo and Lu, Xin and Qian, Shengju and Lu, Jiangbo and Zhang, Xiangyu and Jia, Jiaya},
  year = {2022},
  month = mar,
  number = {arXiv:2112.10175},
  eprint = {2112.10175},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2112.10175},
  urldate = {2023-02-02},
  abstract = {Pre-training has marked numerous state of the arts in high-level computer vision, while few attempts have ever been made to investigate how pre-training acts in image processing systems. In this paper, we tailor transformer-based pre-training regimes that boost various low-level tasks. To comprehensively diagnose the influence of pre-training, we design a whole set of principled evaluation tools that uncover its effects on internal representations. The observations demonstrate that pre-training plays strikingly different roles in low-level tasks. For example, pre-training introduces more local information to higher layers in super-resolution (SR), yielding significant performance gains, while pre-training hardly affects internal feature representations in denoising, resulting in limited gains. Further, we explore different methods of pre-training, revealing that multi-related-task pre-training is more effective and data-efficient than other alternatives. Finally, we extend our study to varying data scales and model sizes, as well as comparisons between transformers and CNNs-based architectures. Based on the study, we successfully develop state-of-the-art models for multiple low-level tasks. Code is released at https://github.com/fenglinglwb/EDT.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,notion},
  file = {G\:\\내 드라이브\\Papers\\On Efficient Transformer-Based Image Pre-training for Low-Level Vision_2022_Li et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\UIK8SJSP\\2112.html}
}

@misc{Liang2020,
  title = {Enhancing {{The Reliability}} of {{Out-of-distribution Image Detection}} in {{Neural Networks}}},
  author = {Liang, Shiyu and Li, Yixuan and Srikant, R.},
  year = {2020},
  month = aug,
  number = {arXiv:1706.02690},
  eprint = {1706.02690},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1706.02690},
  urldate = {2023-06-02},
  abstract = {We consider the problem of detecting out-of-distribution images in neural networks. We propose ODIN, a simple and effective method that does not require any change to a pre-trained neural network. Our method is based on the observation that using temperature scaling and adding small perturbations to the input can separate the softmax score distributions between in- and out-of-distribution images, allowing for more effective detection. We show in a series of experiments that ODIN is compatible with diverse network architectures and datasets. It consistently outperforms the baseline approach by a large margin, establishing a new state-of-the-art performance on this task. For example, ODIN reduces the false positive rate from the baseline 34.7\% to 4.3\% on the DenseNet (applied to CIFAR-10) when the true positive rate is 95\%.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,notion,Statistics - Machine Learning},
  file = {G\:\\내 드라이브\\Papers\\Enhancing The Reliability of Out-of-distribution Image Detection in Neural Networks_2020_Liang et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\EKVKU7UM\\1706.html}
}

@misc{Liang2021,
  title = {{{SwinIR}}: {{Image Restoration Using Swin Transformer}}},
  shorttitle = {{{SwinIR}}},
  author = {Liang, Jingyun and Cao, Jiezhang and Sun, Guolei and Zhang, Kai and Van Gool, Luc and Timofte, Radu},
  year = {2021},
  month = aug,
  number = {arXiv:2108.10257},
  eprint = {2108.10257},
  primaryclass = {cs, eess},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2108.10257},
  urldate = {2023-02-06},
  abstract = {Image restoration is a long-standing low-level vision problem that aims to restore high-quality images from low-quality images (e.g., downscaled, noisy and compressed images). While state-of-the-art image restoration methods are based on convolutional neural networks, few attempts have been made with Transformers which show impressive performance on high-level vision tasks. In this paper, we propose a strong baseline model SwinIR for image restoration based on the Swin Transformer. SwinIR consists of three parts: shallow feature extraction, deep feature extraction and high-quality image reconstruction. In particular, the deep feature extraction module is composed of several residual Swin Transformer blocks (RSTB), each of which has several Swin Transformer layers together with a residual connection. We conduct experiments on three representative tasks: image super-resolution (including classical, lightweight and real-world image super-resolution), image denoising (including grayscale and color image denoising) and JPEG compression artifact reduction. Experimental results demonstrate that SwinIR outperforms state-of-the-art methods on different tasks by \${\textbackslash}textbf\{up to 0.14\${\textbackslash}sim\$0.45dB\}\$, while the total number of parameters can be reduced by \${\textbackslash}textbf\{up to 67\%\}\$.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Electrical Engineering and Systems Science - Image and Video Processing,notion},
  file = {G\:\\내 드라이브\\Papers\\SwinIR - Image Restoration Using Swin Transformer_2021_Liang et al2.pdf;C\:\\Users\\giwon\\Zotero\\storage\\DFYBRZPF\\2108.html}
}

@inproceedings{Liang2021a,
  title = {Swinir: {{Image}} Restoration Using Swin Transformer},
  shorttitle = {Swinir},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF}} International Conference on Computer Vision},
  author = {Liang, Jingyun and Cao, Jiezhang and Sun, Guolei and Zhang, Kai and Van Gool, Luc and Timofte, Radu},
  year = {2021},
  pages = {1833--1844},
  keywords = {notion},
  file = {G\:\\내 드라이브\\Papers\\Swinir - Image restoration using swin transformer_2021_Liang et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\KSDARWJ9\\Liang_SwinIR_Image_Restoration_Using_Swin_Transformer_ICCVW_2021_paper.html}
}

@inproceedings{Lin2014,
  title = {Microsoft Coco: {{Common}} Objects in Context},
  shorttitle = {Microsoft Coco},
  booktitle = {European Conference on Computer Vision},
  author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C. Lawrence},
  year = {2014},
  pages = {740--755},
  publisher = {{Springer}},
  keywords = {notion},
  file = {G\:\\내 드라이브\\Papers\\Microsoft coco - Common objects in context_2014_Lin et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\C4P6QLBL\\978-3-319-10602-1_48.html}
}

@article{Lin2017,
  title = {A Structured Self-Attentive Sentence Embedding},
  author = {Lin, Zhouhan and Feng, Minwei and dos Santos, Cicero Nogueira and Yu, Mo and Xiang, Bing and Zhou, Bowen and Bengio, Yoshua},
  year = {2017},
  journal = {arXiv preprint arXiv:1703.03130},
  eprint = {1703.03130},
  archiveprefix = {arxiv},
  keywords = {notion},
  file = {G\:\\내 드라이브\\Papers\\A structured self-attentive sentence embedding_2017_Lin et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\EM9QMLUW\\1703.html}
}

@inproceedings{Lin2017a,
  title = {Focal Loss for Dense Object Detection},
  booktitle = {Proceedings of the {{IEEE}} International Conference on Computer Vision},
  author = {Lin, Tsung-Yi and Goyal, Priya and Girshick, Ross and He, Kaiming and Doll{\'a}r, Piotr},
  year = {2017},
  pages = {2980--2988},
  keywords = {notion},
  file = {G\:\\내 드라이브\\Papers\\Focal loss for dense object detection_2017_Lin et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\RJBK3Y8Z\\Lin_Focal_Loss_for_ICCV_2017_paper.html}
}

@inproceedings{Liu2021,
  title = {Swin Transformer: {{Hierarchical}} Vision Transformer Using Shifted Windows},
  shorttitle = {Swin Transformer},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF}} International Conference on Computer Vision},
  author = {Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  year = {2021},
  pages = {10012--10022},
  keywords = {notion},
  file = {G\:\\내 드라이브\\Papers\\Swin transformer - Hierarchical vision transformer using shifted windows_2021_Liu et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\2JLBJGNP\\Liu_Swin_Transformer_Hierarchical_Vision_Transformer_Using_Shifted_Windows_ICCV_2021_paper.html}
}

@inproceedings{Liu2022,
  title = {Swin Transformer v2: {{Scaling}} up Capacity and Resolution},
  shorttitle = {Swin Transformer V2},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF}} Conference on Computer Vision and Pattern Recognition},
  author = {Liu, Ze and Hu, Han and Lin, Yutong and Yao, Zhuliang and Xie, Zhenda and Wei, Yixuan and Ning, Jia and Cao, Yue and Zhang, Zheng and Dong, Li},
  year = {2022},
  pages = {12009--12019},
  keywords = {notion},
  file = {G\:\\내 드라이브\\Papers\\Swin transformer v2 - Scaling up capacity and resolution_2022_Liu et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\NX3L859V\\Liu_Swin_Transformer_V2_Scaling_Up_Capacity_and_Resolution_CVPR_2022_paper.html}
}

@inproceedings{Long2015,
  title = {Fully Convolutional Networks for Semantic Segmentation},
  booktitle = {Proceedings of the {{IEEE}} Conference on Computer Vision and Pattern Recognition},
  author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
  year = {2015},
  pages = {3431--3440},
  keywords = {⛔ No DOI found,notion},
  file = {G\:\\내 드라이브\\Papers\\Fully convolutional networks for semantic segmentation_2015_Long et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\2TT5YTDB\\Long_Fully_Convolutional_Networks_2015_CVPR_paper.html}
}

@article{Loshchilov2016,
  title = {Sgdr: {{Stochastic}} Gradient Descent with Warm Restarts},
  shorttitle = {Sgdr},
  author = {Loshchilov, Ilya and Hutter, Frank},
  year = {2016},
  journal = {arXiv preprint arXiv:1608.03983},
  eprint = {1608.03983},
  archiveprefix = {arxiv},
  keywords = {notion},
  file = {G\:\\내 드라이브\\Papers\\Sgdr - Stochastic gradient descent with warm restarts_2016_Loshchilov et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\V6DINNRN\\1608.html}
}

@article{Luo2022,
  title = {Understanding Diffusion Models: {{A}} Unified Perspective},
  shorttitle = {Understanding Diffusion Models},
  author = {Luo, Calvin},
  year = {2022},
  journal = {arXiv preprint arXiv:2208.11970},
  eprint = {2208.11970},
  archiveprefix = {arxiv},
  keywords = {notion},
  file = {G\:\\내 드라이브\\Papers\\Understanding diffusion models - A unified perspective_2022_Luo.pdf;C\:\\Users\\giwon\\Zotero\\storage\\YHXKXMJY\\2208.html}
}

@article{Luong2015,
  title = {Effective Approaches to Attention-Based Neural Machine Translation},
  author = {Luong, Minh-Thang and Pham, Hieu and Manning, Christopher D.},
  year = {2015},
  journal = {arXiv preprint arXiv:1508.04025},
  eprint = {1508.04025},
  archiveprefix = {arxiv},
  keywords = {notion},
  file = {G\:\\내 드라이브\\Papers\\Effective approaches to attention-based neural machine translation_2015_Luong et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\VCC7FLMH\\1508.html}
}

@article{Micikevicius2017,
  title = {Mixed Precision Training},
  author = {Micikevicius, Paulius and Narang, Sharan and Alben, Jonah and Diamos, Gregory and Elsen, Erich and Garcia, David and Ginsburg, Boris and Houston, Michael and Kuchaiev, Oleksii and Venkatesh, Ganesh},
  year = {2017},
  journal = {arXiv preprint arXiv:1710.03740},
  eprint = {1710.03740},
  archiveprefix = {arxiv},
  keywords = {notion},
  file = {G\:\\내 드라이브\\Papers\\Mixed precision training_2017_Micikevicius et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\SAZ9ANMX\\1710.html}
}

@misc{Mildenhall2020,
  title = {{{NeRF}}: {{Representing Scenes}} as {{Neural Radiance Fields}} for {{View Synthesis}}},
  shorttitle = {{{NeRF}}},
  author = {Mildenhall, Ben and Srinivasan, Pratul P. and Tancik, Matthew and Barron, Jonathan T. and Ramamoorthi, Ravi and Ng, Ren},
  year = {2020},
  month = aug,
  number = {arXiv:2003.08934},
  eprint = {2003.08934},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2003.08934},
  urldate = {2022-12-07},
  abstract = {We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location \$(x,y,z)\$ and viewing direction \$({\textbackslash}theta, {\textbackslash}phi)\$) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,notion},
  file = {G\:\\내 드라이브\\Papers\\NeRF - Representing Scenes as Neural Radiance Fields for View Synthesis_2020_Mildenhall et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\FEQXEKHW\\2003.html}
}

@misc{Mirza2014,
  title = {Conditional {{Generative Adversarial Nets}}},
  author = {Mirza, Mehdi and Osindero, Simon},
  year = {2014},
  month = nov,
  number = {arXiv:1411.1784},
  eprint = {1411.1784},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1411.1784},
  urldate = {2023-06-01},
  abstract = {Generative Adversarial Nets [8] were recently introduced as a novel way to train generative models. In this work we introduce the conditional version of generative adversarial nets, which can be constructed by simply feeding the data, y, we wish to condition on to both the generator and discriminator. We show that this model can generate MNIST digits conditioned on class labels. We also illustrate how this model could be used to learn a multi-modal model, and provide preliminary examples of an application to image tagging in which we demonstrate how this approach can generate descriptive tags which are not part of training labels.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,notion,Statistics - Machine Learning},
  file = {G\:\\내 드라이브\\Papers\\Conditional Generative Adversarial Nets_2014_Mirza et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\N9Q2XCAH\\1411.html}
}

@article{Muller2022,
  title = {Instant Neural Graphics Primitives with a Multiresolution Hash Encoding},
  author = {M{\"u}ller, Thomas and Evans, Alex and Schied, Christoph and Keller, Alexander},
  year = {2022},
  journal = {arXiv preprint arXiv:2201.05989},
  eprint = {2201.05989},
  archiveprefix = {arxiv},
  keywords = {notion},
  file = {G\:\\내 드라이브\\Papers\\Instant neural graphics primitives with a multiresolution hash encoding_2022_Müller et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\ZRDFVEV2\\2201.html}
}

@misc{Muller2022a,
  title = {{{DiffRF}}: {{Rendering-Guided 3D Radiance Field Diffusion}}},
  shorttitle = {{{DiffRF}}},
  author = {M{\"u}ller, Norman and Siddiqui, Yawar and Porzi, Lorenzo and Bul{\`o}, Samuel Rota and Kontschieder, Peter and Nie{\ss}ner, Matthias},
  year = {2022},
  month = dec,
  number = {arXiv:2212.01206},
  eprint = {2212.01206},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2212.01206},
  urldate = {2022-12-07},
  abstract = {We introduce DiffRF, a novel approach for 3D radiance field synthesis based on denoising diffusion probabilistic models. While existing diffusion-based methods operate on images, latent codes, or point cloud data, we are the first to directly generate volumetric radiance fields. To this end, we propose a 3D denoising model which directly operates on an explicit voxel grid representation. However, as radiance fields generated from a set of posed images can be ambiguous and contain artifacts, obtaining ground truth radiance field samples is non-trivial. We address this challenge by pairing the denoising formulation with a rendering loss, enabling our model to learn a deviated prior that favours good image quality instead of trying to replicate fitting errors like floating artifacts. In contrast to 2D-diffusion models, our model learns multi-view consistent priors, enabling free-view synthesis and accurate shape generation. Compared to 3D GANs, our diffusion-based approach naturally enables conditional generation such as masked completion or single-view 3D synthesis at inference time.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,notion},
  file = {G\:\\내 드라이브\\Papers\\DiffRF - Rendering-Guided 3D Radiance Field Diffusion_2022_Müller et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\JDDLVQSM\\2212.html}
}

@inproceedings{Munkberg2022,
  title = {Extracting {{Triangular 3D Models}}, {{Materials}}, and {{Lighting From Images}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Munkberg, Jacob and Hasselgren, Jon and Shen, Tianchang and Gao, Jun and Chen, Wenzheng and Evans, Alex and M{\"u}ller, Thomas and Fidler, Sanja},
  year = {2022},
  pages = {8280--8290},
  keywords = {notion},
  file = {G\:\\내 드라이브\\Papers\\Extracting Triangular 3D Models, Materials, and Lighting From Images_2022_Munkberg et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\DH5W359G\\Munkberg_Extracting_Triangular_3D_Models_Materials_and_Lighting_From_Images_CVPR_2022_paper.html}
}

@inproceedings{Nichol2021,
  title = {Improved Denoising Diffusion Probabilistic Models},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Nichol, Alexander Quinn and Dhariwal, Prafulla},
  year = {2021},
  pages = {8162--8171},
  publisher = {{PMLR}},
  keywords = {notion},
  file = {G\:\\내 드라이브\\Papers\\Improved denoising diffusion probabilistic models_2021_Nichol et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\VDDP9UY5\\nichol21a.html}
}

@article{Nichol2022,
  title = {Point-{{E}}: {{A System}} for {{Generating 3D Point Clouds}} from {{Complex Prompts}}},
  shorttitle = {Point-{{E}}},
  author = {Nichol, Alex and Jun, Heewoo and Dhariwal, Prafulla and Mishkin, Pamela and Chen, Mark},
  year = {2022},
  journal = {arXiv preprint arXiv:2212.08751},
  eprint = {2212.08751},
  archiveprefix = {arxiv},
  keywords = {notion},
  file = {G\:\\내 드라이브\\Papers\\Point-E - A System for Generating 3D Point Clouds from Complex Prompts_2022_Nichol et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\IZLB7XP7\\2212.html}
}

@misc{Noh2023,
  title = {{{RankMixup}}: {{Ranking-Based Mixup Training}} for {{Network Calibration}}},
  shorttitle = {{{RankMixup}}},
  author = {Noh, Jongyoun and Park, Hyekang and Lee, Junghyup and Ham, Bumsub},
  year = {2023},
  month = aug,
  number = {arXiv:2308.11990},
  eprint = {2308.11990},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2308.11990},
  urldate = {2023-09-03},
  abstract = {Network calibration aims to accurately estimate the level of confidences, which is particularly important for employing deep neural networks in real-world systems. Recent approaches leverage mixup to calibrate the network's predictions during training. However, they do not consider the problem that mixtures of labels in mixup may not accurately represent the actual distribution of augmented samples. In this paper, we present RankMixup, a novel mixup-based framework alleviating the problem of the mixture of labels for network calibration. To this end, we propose to use an ordinal ranking relationship between raw and mixup-augmented samples as an alternative supervisory signal to the label mixtures for network calibration. We hypothesize that the network should estimate a higher level of confidence for the raw samples than the augmented ones (Fig.1). To implement this idea, we introduce a mixup-based ranking loss (MRL) that encourages lower confidences for augmented samples compared to raw ones, maintaining the ranking relationship. We also propose to leverage the ranking relationship among multiple mixup-augmented samples to further improve the calibration capability. Augmented samples with larger mixing coefficients are expected to have higher confidences and vice versa (Fig.1). That is, the order of confidences should be aligned with that of mixing coefficients. To this end, we introduce a novel loss, M-NDCG, in order to reduce the number of misaligned pairs of the coefficients and confidences. Extensive experimental results on standard benchmarks for network calibration demonstrate the effectiveness of RankMixup.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {G\:\\내 드라이브\\Papers\\RankMixup - Ranking-Based Mixup Training for Network Calibration_2023_Noh et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\N9D8J2JX\\2308.html}
}

@misc{Park2023,
  title = {{{ACLS}}: {{Adaptive}} and {{Conditional Label Smoothing}} for {{Network Calibration}}},
  shorttitle = {{{ACLS}}},
  author = {Park, Hyekang and Noh, Jongyoun and Oh, Youngmin and Baek, Donghyeon and Ham, Bumsub},
  year = {2023},
  month = aug,
  number = {arXiv:2308.11911},
  eprint = {2308.11911},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2308.11911},
  urldate = {2023-09-03},
  abstract = {We address the problem of network calibration adjusting miscalibrated confidences of deep neural networks. Many approaches to network calibration adopt a regularization-based method that exploits a regularization term to smooth the miscalibrated confidences. Although these approaches have shown the effectiveness on calibrating the networks, there is still a lack of understanding on the underlying principles of regularization in terms of network calibration. We present in this paper an in-depth analysis of existing regularization-based methods, providing a better understanding on how they affect to network calibration. Specifically, we have observed that 1) the regularization-based methods can be interpreted as variants of label smoothing, and 2) they do not always behave desirably. Based on the analysis, we introduce a novel loss function, dubbed ACLS, that unifies the merits of existing regularization methods, while avoiding the limitations. We show extensive experimental results for image classification and semantic segmentation on standard benchmarks, including CIFAR10, Tiny-ImageNet, ImageNet, and PASCAL VOC, demonstrating the effectiveness of our loss function.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {G\:\\내 드라이브\\Papers\\ACLS - Adaptive and Conditional Label Smoothing for Network Calibration_2023_Park et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\CQHF473K\\2308.html}
}

@article{Paszke2019,
  title = {Pytorch: {{An}} Imperative Style, High-Performance Deep Learning Library},
  shorttitle = {Pytorch},
  author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca},
  year = {2019},
  journal = {Advances in neural information processing systems},
  volume = {32},
  keywords = {notion},
  file = {G\:\\내 드라이브\\Papers\\Pytorch - An imperative style, high-performance deep learning library_2019_Paszke et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\9YQY6ANV\\bdbca288fee7f92f2bfa9f7012727740-Abstract.html}
}

@misc{Polikar1996,
  title = {The Wavelet Tutorial},
  author = {Polikar, Robi},
  year = {1996},
  file = {G:\내 드라이브\Papers\The wavelet tutorial_1996_Polikar.pdf}
}

@article{Radford2018,
  title = {Improving Language Understanding by Generative Pre-Training},
  author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  year = {2018},
  publisher = {{OpenAI}},
  keywords = {notion},
  file = {G:\내 드라이브\Papers\Improving language understanding by generative pre-training_2018_Radford et al.pdf}
}

@article{Radford2019,
  title = {Language Models Are Unsupervised Multitask Learners},
  author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year = {2019},
  journal = {OpenAI blog},
  volume = {1},
  number = {8},
  pages = {9},
  keywords = {notion},
  file = {G:\내 드라이브\Papers\Language models are unsupervised multitask learners_2019_Radford et al.pdf}
}

@misc{Radford2022,
  title = {Robust {{Speech Recognition}} via {{Large-Scale Weak Supervision}}},
  author = {Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and McLeavey, Christine and Sutskever, Ilya},
  year = {2022},
  month = dec,
  number = {arXiv:2212.04356},
  eprint = {2212.04356},
  primaryclass = {cs, eess},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2212.04356},
  urldate = {2023-12-15},
  abstract = {We study the capabilities of speech processing systems trained simply to predict large amounts of transcripts of audio on the internet. When scaled to 680,000 hours of multilingual and multitask supervision, the resulting models generalize well to standard benchmarks and are often competitive with prior fully supervised results but in a zero-shot transfer setting without the need for any fine-tuning. When compared to humans, the models approach their accuracy and robustness. We are releasing models and inference code to serve as a foundation for further work on robust speech processing.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {G\:\\내 드라이브\\Papers\\Robust Speech Recognition via Large-Scale Weak Supervision_2022_Radford et al2.pdf;C\:\\Users\\giwon\\Zotero\\storage\\6YK34ZPA\\2212.html}
}

@article{Radford2022a,
  title = {Robust Speech Recognition via Large-Scale Weak Supervision},
  author = {Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and McLeavey, Christine and Sutskever, Ilya},
  year = {2022},
  journal = {arXiv preprint arXiv:2212.04356},
  eprint = {2212.04356},
  archiveprefix = {arxiv},
  keywords = {notion},
  file = {G\:\\내 드라이브\\Papers\\Robust speech recognition via large-scale weak supervision_2022_Radford et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\FNMVSZBD\\2212.html}
}

@misc{Rastegari2016,
  title = {{{XNOR-Net}}: {{ImageNet Classification Using Binary Convolutional Neural Networks}}},
  shorttitle = {{{XNOR-Net}}},
  author = {Rastegari, Mohammad and Ordonez, Vicente and Redmon, Joseph and Farhadi, Ali},
  year = {2016},
  month = aug,
  number = {arXiv:1603.05279},
  eprint = {1603.05279},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-09-25},
  abstract = {We propose two efficient approximations to standard convolutional neural networks: Binary-Weight-Networks and XNOR-Networks. In Binary-Weight-Networks, the filters are approximated with binary values resulting in 32x memory saving. In XNOR-Networks, both the filters and the input to convolutional layers are binary. XNOR-Networks approximate convolutions using primarily binary operations. This results in 58x faster convolutional operations and 32x memory savings. XNOR-Nets offer the possibility of running state-of-the-art networks on CPUs (rather than GPUs) in real-time. Our binary networks are simple, accurate, efficient, and work on challenging visual tasks. We evaluate our approach on the ImageNet classification task. The classification accuracy with a Binary-Weight-Network version of AlexNet is only 2.9\% less than the full-precision AlexNet (in top-1 measure). We compare our method with recent network binarization methods, BinaryConnect and BinaryNets, and outperform these methods by large margins on ImageNet, more than 16\% in top-1 accuracy.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {G\:\\내 드라이브\\Papers\\XNOR-Net - ImageNet Classification Using Binary Convolutional Neural Networks_2016_Rastegari et al2.pdf;C\:\\Users\\giwon\\Zotero\\storage\\UIAUQBGN\\1603.html}
}

@misc{Rastegari2016a,
  title = {{{XNOR-Net}}: {{ImageNet Classification Using Binary Convolutional Neural Networks}}},
  shorttitle = {{{XNOR-Net}}},
  author = {Rastegari, Mohammad and Ordonez, Vicente and Redmon, Joseph and Farhadi, Ali},
  year = {2016},
  month = aug,
  number = {arXiv:1603.05279},
  eprint = {1603.05279},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1603.05279},
  urldate = {2023-09-04},
  abstract = {We propose two efficient approximations to standard convolutional neural networks: Binary-Weight-Networks and XNOR-Networks. In Binary-Weight-Networks, the filters are approximated with binary values resulting in 32x memory saving. In XNOR-Networks, both the filters and the input to convolutional layers are binary. XNOR-Networks approximate convolutions using primarily binary operations. This results in 58x faster convolutional operations and 32x memory savings. XNOR-Nets offer the possibility of running state-of-the-art networks on CPUs (rather than GPUs) in real-time. Our binary networks are simple, accurate, efficient, and work on challenging visual tasks. We evaluate our approach on the ImageNet classification task. The classification accuracy with a Binary-Weight-Network version of AlexNet is only 2.9\% less than the full-precision AlexNet (in top-1 measure). We compare our method with recent network binarization methods, BinaryConnect and BinaryNets, and outperform these methods by large margins on ImageNet, more than 16\% in top-1 accuracy.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,notion},
  file = {G\:\\내 드라이브\\Papers\\XNOR-Net - ImageNet Classification Using Binary Convolutional Neural Networks_2016_Rastegari et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\6NPMB9MA\\1603.html}
}

@inproceedings{Redmon2016,
  title = {You Only Look Once: {{Unified}}, Real-Time Object Detection},
  shorttitle = {You Only Look Once},
  booktitle = {Proceedings of the {{IEEE}} Conference on Computer Vision and Pattern Recognition},
  author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
  year = {2016},
  pages = {779--788},
  keywords = {notion},
  file = {G\:\\내 드라이브\\Papers\\You only look once - Unified, real-time object detection_2016_Redmon et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\LB6SD65S\\Redmon_You_Only_Look_CVPR_2016_paper.html}
}

@article{Redmon2018,
  title = {Yolov3: {{An}} Incremental Improvement},
  shorttitle = {Yolov3},
  author = {Redmon, Joseph and Farhadi, Ali},
  year = {2018},
  journal = {arXiv preprint arXiv:1804.02767},
  eprint = {1804.02767},
  archiveprefix = {arxiv},
  keywords = {notion},
  file = {G\:\\내 드라이브\\Papers\\Yolov3 - An incremental improvement_2018_Redmon et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\WRXELAFC\\1804.html}
}

@article{Ren2015,
  title = {Faster R-Cnn: {{Towards}} Real-Time Object Detection with Region Proposal Networks},
  shorttitle = {Faster R-Cnn},
  author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
  year = {2015},
  journal = {Advances in neural information processing systems},
  volume = {28},
  keywords = {notion},
  file = {G\:\\내 드라이브\\Papers\\Faster r-cnn - Towards real-time object detection with region proposal networks_2015_Ren et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\S27AWYZ3\\14bfa6bb14875e45bba028a21ed38046-Abstract.html}
}

@inproceedings{Rombach2022,
  title = {High-Resolution Image Synthesis with Latent Diffusion Models},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj{\"o}rn},
  year = {2022},
  pages = {10684--10695},
  keywords = {notion},
  file = {G\:\\내 드라이브\\Papers\\High-resolution image synthesis with latent diffusion models_2022_Rombach et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\IKP3VVVW\\Rombach_High-Resolution_Image_Synthesis_With_Latent_Diffusion_Models_CVPR_2022_paper.html}
}

@inproceedings{Ronneberger2015,
  title = {U-Net: {{Convolutional}} Networks for Biomedical Image Segmentation},
  shorttitle = {U-Net},
  booktitle = {International {{Conference}} on {{Medical}} Image Computing and Computer-Assisted Intervention},
  author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  year = {2015},
  pages = {234--241},
  publisher = {{Springer}},
  keywords = {notion},
  file = {G\:\\내 드라이브\\Papers\\U-net - Convolutional networks for biomedical image segmentation_2015_Ronneberger et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\8QLFGK33\\978-3-319-24574-4_28.html}
}

@article{Rother2004,
  title = {" {{GrabCut}}" Interactive Foreground Extraction Using Iterated Graph Cuts},
  author = {Rother, Carsten and Kolmogorov, Vladimir and Blake, Andrew},
  year = {2004},
  journal = {ACM transactions on graphics (TOG)},
  volume = {23},
  number = {3},
  pages = {309--314},
  publisher = {{ACM New York, NY, USA}},
  keywords = {notion},
  file = {G\:\\내 드라이브\\Papers\\GrabCut interactive foreground extraction using iterated graph cuts_2004_Rother et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\U3IRFXXR\\1015706.html}
}

@article{Saharia2022,
  title = {Image Super-Resolution via Iterative Refinement},
  author = {Saharia, Chitwan and Ho, Jonathan and Chan, William and Salimans, Tim and Fleet, David J. and Norouzi, Mohammad},
  year = {2022},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  publisher = {{IEEE}},
  keywords = {notion},
  file = {C:\Users\giwon\Zotero\storage\TLVHRR5Q\9887996.html}
}

@article{Salton1988,
  title = {Term-Weighting Approaches in Automatic Text Retrieval},
  author = {Salton, Gerard and Buckley, Christopher},
  year = {1988},
  journal = {Information processing \& management},
  volume = {24},
  number = {5},
  pages = {513--523},
  publisher = {{Elsevier}},
  doi = {10.1016/0306-4573(88)90021-0},
  urldate = {2023-10-04},
  file = {G:\내 드라이브\Papers\Term-weighting approaches in automatic text retrieval_1988_Salton et al.pdf}
}

@article{Sauer2023,
  title = {{{StyleGAN-T}}: {{Unlocking}} the {{Power}} of {{GANs}} for {{Fast Large-Scale Text-to-Image Synthesis}}},
  shorttitle = {{{StyleGAN-T}}},
  author = {Sauer, Axel and Karras, Tero and Laine, Samuli and Geiger, Andreas and Aila, Timo},
  year = {2023},
  journal = {arXiv preprint arXiv:2301.09515},
  eprint = {2301.09515},
  archiveprefix = {arxiv},
  keywords = {notion},
  file = {G\:\\내 드라이브\\Papers\\StyleGAN-T - Unlocking the Power of GANs for Fast Large-Scale Text-to-Image_2023_Sauer et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\DGN2MEDT\\2301.html}
}

@article{Selvaraju2020,
  title = {Grad-{{CAM}}: {{Visual Explanations}} from {{Deep Networks}} via {{Gradient-based Localization}}},
  shorttitle = {Grad-{{CAM}}},
  author = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
  year = {2020},
  month = feb,
  journal = {International Journal of Computer Vision},
  volume = {128},
  number = {2},
  eprint = {1610.02391},
  primaryclass = {cs},
  pages = {336--359},
  issn = {0920-5691, 1573-1405},
  doi = {10.1007/s11263-019-01228-7},
  urldate = {2023-04-26},
  abstract = {We propose a technique for producing "visual explanations" for decisions from a large class of CNN-based models, making them more transparent. Our approach - Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept, flowing into the final convolutional layer to produce a coarse localization map highlighting important regions in the image for predicting the concept. Grad-CAM is applicable to a wide variety of CNN model-families: (1) CNNs with fully-connected layers, (2) CNNs used for structured outputs, (3) CNNs used in tasks with multimodal inputs or reinforcement learning, without any architectural changes or re-training. We combine Grad-CAM with fine-grained visualizations to create a high-resolution class-discriminative visualization and apply it to off-the-shelf image classification, captioning, and visual question answering (VQA) models, including ResNet-based architectures. In the context of image classification models, our visualizations (a) lend insights into their failure modes, (b) are robust to adversarial images, (c) outperform previous methods on localization, (d) are more faithful to the underlying model and (e) help achieve generalization by identifying dataset bias. For captioning and VQA, we show that even non-attention based models can localize inputs. We devise a way to identify important neurons through Grad-CAM and combine it with neuron names to provide textual explanations for model decisions. Finally, we design and conduct human studies to measure if Grad-CAM helps users establish appropriate trust in predictions from models and show that Grad-CAM helps untrained users successfully discern a 'stronger' nodel from a 'weaker' one even when both make identical predictions. Our code is available at https://github.com/ramprs/grad-cam/, along with a demo at http://gradcam.cloudcv.org, and a video at youtu.be/COjUB9Izk6E.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,notion},
  file = {G\:\\내 드라이브\\Papers\\Grad-CAM - Visual Explanations from Deep Networks via Gradient-based Localization_2020_Selvaraju et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\B84DY7H4\\1610.html}
}

@article{Shao2022,
  title = {Safety-Enhanced Autonomous Driving Using Interpretable Sensor Fusion Transformer},
  author = {Shao, Hao and Wang, Letian and Chen, Ruobing and Li, Hongsheng and Liu, Yu},
  year = {2022},
  journal = {arXiv preprint arXiv:2207.14024},
  eprint = {2207.14024},
  archiveprefix = {arxiv},
  keywords = {notion},
  file = {G\:\\내 드라이브\\Papers\\Safety-enhanced autonomous driving using interpretable sensor fusion transformer_2022_Shao et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\SHNSY4DE\\2207.html}
}

@article{Sherstinsky2020,
  title = {Fundamentals of {{Recurrent Neural Network}} ({{RNN}}) and {{Long Short-Term Memory}} ({{LSTM}}) {{Network}}},
  author = {Sherstinsky, Alex},
  year = {2020},
  month = mar,
  journal = {Physica D: Nonlinear Phenomena},
  volume = {404},
  eprint = {1808.03314},
  primaryclass = {cs, stat},
  pages = {132306},
  issn = {01672789},
  doi = {10.1016/j.physd.2019.132306},
  urldate = {2023-05-09},
  abstract = {Because of their effectiveness in broad practical applications, LSTM networks have received a wealth of coverage in scientific journals, technical blogs, and implementation guides. However, in most articles, the inference formulas for the LSTM network and its parent, RNN, are stated axiomatically, while the training formulas are omitted altogether. In addition, the technique of "unrolling" an RNN is routinely presented without justification throughout the literature. The goal of this paper is to explain the essential RNN and LSTM fundamentals in a single document. Drawing from concepts in signal processing, we formally derive the canonical RNN formulation from differential equations. We then propose and prove a precise statement, which yields the RNN unrolling technique. We also review the difficulties with training the standard RNN and address them by transforming the RNN into the "Vanilla LSTM" network through a series of logical arguments. We provide all equations pertaining to the LSTM system together with detailed descriptions of its constituent entities. Albeit unconventional, our choice of notation and the method for presenting the LSTM system emphasizes ease of understanding. As part of the analysis, we identify new opportunities to enrich the LSTM system and incorporate these extensions into the Vanilla LSTM network, producing the most general LSTM variant to date. The target reader has already been exposed to RNNs and LSTM networks through numerous available resources and is open to an alternative pedagogical approach. A Machine Learning practitioner seeking guidance for implementing our new augmented LSTM model in software for experimentation and research will find the insights and derivations in this tutorial valuable as well.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,notion,Statistics - Machine Learning},
  file = {G\:\\내 드라이브\\Papers\\Fundamentals of Recurrent Neural Network (RNN) and Long Short-Term Memory_2020_Sherstinsky.pdf;C\:\\Users\\giwon\\Zotero\\storage\\HGRSD86Y\\1808.html}
}

@misc{Shoshan2019,
  title = {Dynamic-{{Net}}: {{Tuning}} the {{Objective Without Re-training}} for {{Synthesis Tasks}}},
  shorttitle = {Dynamic-{{Net}}},
  author = {Shoshan, Alon and Mechrez, Roey and {Zelnik-Manor}, Lihi},
  year = {2019},
  month = aug,
  number = {arXiv:1811.08760},
  eprint = {1811.08760},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1811.08760},
  urldate = {2023-09-08},
  abstract = {One of the key ingredients for successful optimization of modern CNNs is identifying a suitable objective. To date, the objective is fixed a-priori at training time, and any variation to it requires re-training a new network. In this paper we present a first attempt at alleviating the need for re-training. Rather than fixing the network at training time, we train a "Dynamic-Net" that can be modified at inference time. Our approach considers an "objective-space" as the space of all linear combinations of two objectives, and the Dynamic-Net is emulating the traversing of this objective-space at test-time, without any further training. We show that this upgrades pre-trained networks by providing an out-of-learning extension, while maintaining the performance quality. The solution we propose is fast and allows a user to interactively modify the network, in real-time, in order to obtain the result he/she desires. We show the benefits of such an approach via several different applications.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,notion},
  file = {G\:\\내 드라이브\\Papers\\Dynamic-Net - Tuning the Objective Without Re-training for Synthesis Tasks_2019_Shoshan et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\KTRWXIM8\\1811.html}
}

@inproceedings{Sievert2014,
  title = {{{LDAvis}}: {{A}} Method for Visualizing and Interpreting Topics},
  shorttitle = {{{LDAvis}}},
  booktitle = {Proceedings of the Workshop on Interactive Language Learning, Visualization, and Interfaces},
  author = {Sievert, Carson and Shirley, Kenneth},
  year = {2014},
  pages = {63--70},
  doi = {10.3115/v1/W14-3110},
  urldate = {2023-10-04},
  file = {G:\내 드라이브\Papers\LDAvis - A method for visualizing and interpreting topics_2014_Sievert et al.pdf}
}

@misc{Simonyan2015,
  title = {Very {{Deep Convolutional Networks}} for {{Large-Scale Image Recognition}}},
  author = {Simonyan, Karen and Zisserman, Andrew},
  year = {2015},
  month = apr,
  number = {arXiv:1409.1556},
  eprint = {1409.1556},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1409.1556},
  urldate = {2023-03-14},
  abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,notion},
  file = {G\:\\내 드라이브\\Papers\\Very Deep Convolutional Networks for Large-Scale Image Recognition_2015_Simonyan et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\GBBF4SBL\\1409.html}
}

@inproceedings{Sohl-Dickstein2015,
  title = {Deep Unsupervised Learning Using Nonequilibrium Thermodynamics},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {{Sohl-Dickstein}, Jascha and Weiss, Eric and Maheswaranathan, Niru and Ganguli, Surya},
  year = {2015},
  pages = {2256--2265},
  publisher = {{PMLR}},
  keywords = {notion},
  file = {G\:\\내 드라이브\\Papers\\Deep unsupervised learning using nonequilibrium thermodynamics_2015_Sohl-Dickstein et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\YFDK78DT\\sohl-dickstein15.html}
}

@article{Song2023,
  title = {Consistency {{Models}}},
  author = {Song, Yang and Dhariwal, Prafulla and Chen, Mark and Sutskever, Ilya},
  year = {2023},
  journal = {arXiv preprint arXiv:2303.01469},
  eprint = {2303.01469},
  archiveprefix = {arxiv},
  keywords = {⛔ No DOI found,notion},
  file = {C:\Users\giwon\Zotero\storage\ESX988W6\2303.html}
}

@inproceedings{Soudry2014,
  title = {Expectation {{Backpropagation}}: {{Parameter-Free Training}} of {{Multilayer Neural Networks}} with {{Continuous}} or {{Discrete Weights}}},
  shorttitle = {Expectation {{Backpropagation}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Soudry, Daniel and Hubara, Itay and Meir, Ron},
  year = {2014},
  volume = {27},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2023-09-18},
  abstract = {Multilayer Neural Networks (MNNs) are commonly trained using gradient descent-based methods, such as BackPropagation (BP). Inference in probabilistic graphical models is often done using variational Bayes methods, such as Expectation Propagation (EP). We show how an EP based approach can also be used to train deterministic MNNs. Specifically, we approximate the posterior of the weights given the data using a ``mean-field'' factorized distribution, in an online setting. Using online EP and the central limit theorem we find an analytical approximation to the Bayes update of this posterior, as well as the resulting Bayes estimates of the weights and outputs. Despite a different origin, the resulting algorithm, Expectation BackPropagation (EBP), is very similar to BP in form and efficiency. However, it has several additional advantages: (1) Training is parameter-free, given initial conditions (prior) and the MNN architecture. This is useful for large-scale problems, where parameter tuning is a major challenge. (2) The weights can be restricted to have discrete values. This is especially useful for implementing trained MNNs in precision limited hardware chips, thus improving their speed and energy efficiency by several orders of magnitude. We test the EBP algorithm numerically in eight binary text classification tasks. In all tasks, EBP outperforms: (1) standard BP with the optimal constant learning rate (2) previously reported state of the art. Interestingly, EBP-trained MNNs with binary weights usually perform better than MNNs with continuous (real) weights - if we average the MNN output using the inferred posterior.},
  keywords = {⛔ No DOI found},
  file = {G:\내 드라이브\Papers\Expectation Backpropagation - Parameter-Free Training of Multilayer Neural_2014_Soudry et al.pdf}
}

@incollection{Sudre2017,
  title = {Generalised Dice Overlap as a Deep Learning Loss Function for Highly Unbalanced Segmentations},
  booktitle = {Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support},
  author = {Sudre, Carole H. and Li, Wenqi and Vercauteren, Tom and Ourselin, Sebastien and Jorge Cardoso, M.},
  year = {2017},
  pages = {240--248},
  publisher = {{Springer}},
  keywords = {notion},
  file = {G\:\\내 드라이브\\Papers\\Generalised dice overlap as a deep learning loss function for highly unbalanced_2017_Sudre et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\EMMHV9J5\\978-3-319-67558-9_28.html;C\:\\Users\\giwon\\Zotero\\storage\\GSSLKHVT\\PMC7610921.html}
}

@inproceedings{Sun2021,
  title = {Task Programming: {{Learning}} Data Efficient Behavior Representations},
  shorttitle = {Task Programming},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Sun, Jennifer J. and Kennedy, Ann and Zhan, Eric and Anderson, David J. and Yue, Yisong and Perona, Pietro},
  year = {2021},
  pages = {2876--2885},
  keywords = {notion},
  file = {G\:\\내 드라이브\\Papers\\Task programming - Learning data efficient behavior representations_2021_Sun et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\MWURAT3N\\Sun_Task_Programming_Learning_Data_Efficient_Behavior_Representations_CVPR_2021_paper.html}
}

@misc{Sutskever2014,
  title = {Sequence to {{Sequence Learning}} with {{Neural Networks}}},
  author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V.},
  year = {2014},
  month = dec,
  number = {arXiv:1409.3215},
  eprint = {1409.3215},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1409.3215},
  urldate = {2023-05-18},
  abstract = {Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT'14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous best result on this task. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,notion},
  file = {G\:\\내 드라이브\\Papers\\Sequence to Sequence Learning with Neural Networks_2014_Sutskever et al2.pdf;C\:\\Users\\giwon\\Zotero\\storage\\AHEUXYLM\\1409.html}
}

@article{Sutskever2014a,
  title = {Sequence to Sequence Learning with Neural Networks},
  author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V.},
  year = {2014},
  journal = {Advances in neural information processing systems},
  volume = {27},
  keywords = {notion},
  file = {G\:\\내 드라이브\\Papers\\Sequence to sequence learning with neural networks_2014_Sutskever et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\X6KVIM2A\\a14ac55a4f27472c5d894ec1c3c743d2-Abstract.html}
}

@inproceedings{Takikawa2021,
  title = {Neural Geometric Level of Detail: {{Real-time}} Rendering with Implicit {{3D}} Shapes},
  shorttitle = {Neural Geometric Level of Detail},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Takikawa, Towaki and Litalien, Joey and Yin, Kangxue and Kreis, Karsten and Loop, Charles and Nowrouzezahrai, Derek and Jacobson, Alec and McGuire, Morgan and Fidler, Sanja},
  year = {2021},
  pages = {11358--11367},
  keywords = {notion},
  file = {G\:\\내 드라이브\\Papers\\Neural geometric level of detail - Real-time rendering with implicit 3D shapes_2021_Takikawa et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\62BXDELL\\Takikawa_Neural_Geometric_Level_of_Detail_Real-Time_Rendering_With_Implicit_3D_CVPR_2021_paper.html}
}

@inproceedings{Tan2019,
  title = {Efficientnet: {{Rethinking}} Model Scaling for Convolutional Neural Networks},
  shorttitle = {Efficientnet},
  booktitle = {International Conference on Machine Learning},
  author = {Tan, Mingxing and Le, Quoc},
  year = {2019},
  pages = {6105--6114},
  publisher = {{PMLR}},
  keywords = {notion},
  file = {G\:\\내 드라이브\\Papers\\Efficientnet - Rethinking model scaling for convolutional neural networks_2019_Tan et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\42XU2BF5\\tan19a.html}
}

@misc{Tan2021,
  title = {{{EfficientNetV2}}: {{Smaller Models}} and {{Faster Training}}},
  shorttitle = {{{EfficientNetV2}}},
  author = {Tan, Mingxing and Le, Quoc V.},
  year = {2021},
  month = jun,
  number = {arXiv:2104.00298},
  eprint = {2104.00298},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2104.00298},
  urldate = {2022-12-07},
  abstract = {This paper introduces EfficientNetV2, a new family of convolutional networks that have faster training speed and better parameter efficiency than previous models. To develop this family of models, we use a combination of training-aware neural architecture search and scaling, to jointly optimize training speed and parameter efficiency. The models were searched from the search space enriched with new ops such as Fused-MBConv. Our experiments show that EfficientNetV2 models train much faster than state-of-the-art models while being up to 6.8x smaller. Our training can be further sped up by progressively increasing the image size during training, but it often causes a drop in accuracy. To compensate for this accuracy drop, we propose to adaptively adjust regularization (e.g., dropout and data augmentation) as well, such that we can achieve both fast training and good accuracy. With progressive learning, our EfficientNetV2 significantly outperforms previous models on ImageNet and CIFAR/Cars/Flowers datasets. By pretraining on the same ImageNet21k, our EfficientNetV2 achieves 87.3\% top-1 accuracy on ImageNet ILSVRC2012, outperforming the recent ViT by 2.0\% accuracy while training 5x-11x faster using the same computing resources. Code will be available at https://github.com/google/automl/tree/master/efficientnetv2.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,notion},
  file = {G\:\\내 드라이브\\Papers\\EfficientNetV2 - Smaller Models and Faster Training_2021_Tan et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\NJFZH8QS\\2104.html}
}

@article{Tarvainen2017,
  title = {Mean Teachers Are Better Role Models: {{Weight-averaged}} Consistency Targets Improve Semi-Supervised Deep Learning Results},
  shorttitle = {Mean Teachers Are Better Role Models},
  author = {Tarvainen, Antti and Valpola, Harri},
  year = {2017},
  journal = {Advances in neural information processing systems},
  volume = {30},
  keywords = {notion},
  file = {G\:\\내 드라이브\\Papers\\Mean teachers are better role models - Weight-averaged consistency targets_2017_Tarvainen et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\IEUZKRSF\\68053af2923e00204c3ca7c6a3150cf7-Abstract.html}
}

@misc{Teed2020,
  title = {{{RAFT}}: {{Recurrent All-Pairs Field Transforms}} for {{Optical Flow}}},
  shorttitle = {{{RAFT}}},
  author = {Teed, Zachary and Deng, Jia},
  year = {2020},
  month = aug,
  number = {arXiv:2003.12039},
  eprint = {2003.12039},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2003.12039},
  urldate = {2023-05-16},
  abstract = {We introduce Recurrent All-Pairs Field Transforms (RAFT), a new deep network architecture for optical flow. RAFT extracts per-pixel features, builds multi-scale 4D correlation volumes for all pairs of pixels, and iteratively updates a flow field through a recurrent unit that performs lookups on the correlation volumes. RAFT achieves state-of-the-art performance. On KITTI, RAFT achieves an F1-all error of 5.10\%, a 16\% error reduction from the best published result (6.10\%). On Sintel (final pass), RAFT obtains an end-point-error of 2.855 pixels, a 30\% error reduction from the best published result (4.098 pixels). In addition, RAFT has strong cross-dataset generalization as well as high efficiency in inference time, training speed, and parameter count. Code is available at https://github.com/princeton-vl/RAFT.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,notion},
  file = {G\:\\내 드라이브\\Papers\\RAFT - Recurrent All-Pairs Field Transforms for Optical Flow_2020_Teed et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\ZC2JW34Q\\2003.html}
}

@article{Torrence1998,
  title = {A Practical Guide to Wavelet Analysis},
  author = {Torrence, Christopher and Compo, Gilbert P.},
  year = {1998},
  journal = {Bulletin of the American Meteorological society},
  volume = {79},
  number = {1},
  pages = {61--78},
  publisher = {{American Meteorological Society}},
  doi = {10.1175/1520-0477(1998)079<0061:APGTWA>2.0.CO;2},
  file = {C:\Users\giwon\Zotero\storage\ETCPHX5T\1520-0477_1998_079_0061_apgtwa_2_0_co_2.html}
}

@article{Tumanyan2022,
  title = {Plug-and-{{Play Diffusion Features}} for {{Text-Driven Image-to-Image Translation}}},
  author = {Tumanyan, Narek and Geyer, Michal and Bagon, Shai and Dekel, Tali},
  year = {2022},
  journal = {arXiv preprint arXiv:2211.12572},
  eprint = {2211.12572},
  archiveprefix = {arxiv},
  keywords = {notion},
  file = {C:\Users\giwon\Zotero\storage\943RRS32\2211.html}
}

@article{Vaswani2017,
  title = {Attention Is All You Need},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, {\textbackslash}Lukasz and Polosukhin, Illia},
  year = {2017},
  journal = {Advances in neural information processing systems},
  volume = {30},
  keywords = {notion},
  file = {G\:\\내 드라이브\\Papers\\Attention is all you need_2017_Vaswani et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\5MFZZGZM\\3f5ee243547dee91fbd053c1c4a845aa-Abstract.html}
}

@article{Wang2022,
  title = {Rodin: {{A Generative Model}} for {{Sculpting 3D Digital Avatars Using Diffusion}}},
  shorttitle = {Rodin},
  author = {Wang, Tengfei and Zhang, Bo and Zhang, Ting and Gu, Shuyang and Bao, Jianmin and Baltrusaitis, Tadas and Shen, Jingjing and Chen, Dong and Wen, Fang and Chen, Qifeng},
  year = {2022},
  journal = {arXiv preprint arXiv:2212.06135},
  eprint = {2212.06135},
  archiveprefix = {arxiv},
  keywords = {notion},
  file = {G\:\\내 드라이브\\Papers\\Rodin - A Generative Model for Sculpting 3D Digital Avatars Using Diffusion_2022_Wang et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\N9MBCQFC\\2212.html}
}

@article{Watson2021,
  title = {Learning to Efficiently Sample from Diffusion Probabilistic Models},
  author = {Watson, Daniel and Ho, Jonathan and Norouzi, Mohammad and Chan, William},
  year = {2021},
  journal = {arXiv preprint arXiv:2106.03802},
  eprint = {2106.03802},
  archiveprefix = {arxiv},
  keywords = {notion},
  file = {G\:\\내 드라이브\\Papers\\Learning to efficiently sample from diffusion probabilistic models_2021_Watson et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\4Q6Y2V52\\2106.html}
}

@inproceedings{Welling2011,
  title = {Bayesian Learning via Stochastic Gradient {{Langevin}} Dynamics},
  booktitle = {Proceedings of the 28th International Conference on Machine Learning ({{ICML-11}})},
  author = {Welling, Max and Teh, Yee W.},
  year = {2011},
  pages = {681--688},
  keywords = {notion},
  file = {G:\내 드라이브\Papers\Bayesian learning via stochastic gradient Langevin dynamics_2011_Welling et al.pdf}
}

@article{Wronski2019,
  title = {Handheld {{Multi-Frame Super-Resolution}}},
  author = {Wronski, Bartlomiej and {Garcia-Dorado}, Ignacio and Ernst, Manfred and Kelly, Damien and Krainin, Michael and Liang, Chia-Kai and Levoy, Marc and Milanfar, Peyman},
  year = {2019},
  month = aug,
  journal = {ACM Transactions on Graphics},
  volume = {38},
  number = {4},
  eprint = {1905.03277},
  primaryclass = {cs, eess},
  pages = {1--18},
  issn = {0730-0301, 1557-7368},
  doi = {10.1145/3306346.3323024},
  urldate = {2023-02-06},
  abstract = {Compared to DSLR cameras, smartphone cameras have smaller sensors, which limits their spatial resolution; smaller apertures, which limits their light gathering ability; and smaller pixels, which reduces their signal-to noise ratio. The use of color filter arrays (CFAs) requires demosaicing, which further degrades resolution. In this paper, we supplant the use of traditional demosaicing in single-frame and burst photography pipelines with a multiframe super-resolution algorithm that creates a complete RGB image directly from a burst of CFA raw images. We harness natural hand tremor, typical in handheld photography, to acquire a burst of raw frames with small offsets. These frames are then aligned and merged to form a single image with red, green, and blue values at every pixel site. This approach, which includes no explicit demosaicing step, serves to both increase image resolution and boost signal to noise ratio. Our algorithm is robust to challenging scene conditions: local motion, occlusion, or scene changes. It runs at 100 milliseconds per 12-megapixel RAW input burst frame on mass-produced mobile phones. Specifically, the algorithm is the basis of the Super-Res Zoom feature, as well as the default merge method in Night Sight mode (whether zooming or not) on Google's flagship phone.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Electrical Engineering and Systems Science - Image and Video Processing,notion},
  file = {G\:\\내 드라이브\\Papers\\Handheld Multi-Frame Super-Resolution_2019_Wronski et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\I6AWMPXH\\1905.html}
}

@misc{Xiaohei2023,
  title = {{{0Xiaohei0}}/{{VoiceToJapanese}}},
  author = {Xiaohei},
  year = {2023},
  month = dec,
  urldate = {2023-12-15}
}

@misc{Yin2019,
  title = {Understanding {{Straight-Through Estimator}} in {{Training Activation Quantized Neural Nets}}},
  author = {Yin, Penghang and Lyu, Jiancheng and Zhang, Shuai and Osher, Stanley and Qi, Yingyong and Xin, Jack},
  year = {2019},
  month = sep,
  number = {arXiv:1903.05662},
  eprint = {1903.05662},
  primaryclass = {cs, math, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1903.05662},
  urldate = {2023-09-21},
  abstract = {Training activation quantized neural networks involves minimizing a piecewise constant function whose gradient vanishes almost everywhere, which is undesirable for the standard back-propagation or chain rule. An empirical way around this issue is to use a straight-through estimator (STE) (Bengio et al., 2013) in the backward pass only, so that the "gradient" through the modified chain rule becomes non-trivial. Since this unusual "gradient" is certainly not the gradient of loss function, the following question arises: why searching in its negative direction minimizes the training loss? In this paper, we provide the theoretical justification of the concept of STE by answering this question. We consider the problem of learning a two-linear-layer network with binarized ReLU activation and Gaussian input data. We shall refer to the unusual "gradient" given by the STE-modifed chain rule as coarse gradient. The choice of STE is not unique. We prove that if the STE is properly chosen, the expected coarse gradient correlates positively with the population gradient (not available for the training), and its negation is a descent direction for minimizing the population loss. We further show the associated coarse gradient descent algorithm converges to a critical point of the population loss minimization problem. Moreover, we show that a poor choice of STE leads to instability of the training algorithm near certain local minima, which is verified with CIFAR-10 experiments.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,notion,Statistics - Machine Learning},
  file = {G\:\\내 드라이브\\Papers\\Understanding Straight-Through Estimator in Training Activation Quantized_2019_Yin et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\9HTVLU3G\\1903.html}
}

@article{Yu2015,
  title = {Multi-Scale Context Aggregation by Dilated Convolutions},
  author = {Yu, Fisher and Koltun, Vladlen},
  year = {2015},
  journal = {arXiv preprint arXiv:1511.07122},
  eprint = {1511.07122},
  archiveprefix = {arxiv},
  keywords = {⛔ No DOI found,notion},
  file = {G\:\\내 드라이브\\Papers\\Multi-scale context aggregation by dilated convolutions_2015_Yu et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\4XUMYB6H\\1511.html}
}

@misc{Yu2018,
  title = {Slimmable {{Neural Networks}}},
  author = {Yu, Jiahui and Yang, Linjie and Xu, Ning and Yang, Jianchao and Huang, Thomas},
  year = {2018},
  month = dec,
  number = {arXiv:1812.08928},
  eprint = {1812.08928},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1812.08928},
  urldate = {2023-09-08},
  abstract = {We present a simple and general method to train a single neural network executable at different widths (number of channels in a layer), permitting instant and adaptive accuracy-efficiency trade-offs at runtime. Instead of training individual networks with different width configurations, we train a shared network with switchable batch normalization. At runtime, the network can adjust its width on the fly according to on-device benchmarks and resource constraints, rather than downloading and offloading different models. Our trained networks, named slimmable neural networks, achieve similar (and in many cases better) ImageNet classification accuracy than individually trained models of MobileNet v1, MobileNet v2, ShuffleNet and ResNet-50 at different widths respectively. We also demonstrate better performance of slimmable models compared with individual ones across a wide range of applications including COCO bounding-box object detection, instance segmentation and person keypoint detection without tuning hyper-parameters. Lastly we visualize and discuss the learned features of slimmable networks. Code and models are available at: https://github.com/JiahuiYu/slimmable\_networks},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,notion},
  file = {G\:\\내 드라이브\\Papers\\Slimmable Neural Networks_2018_Yu et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\A5LGQ596\\1812.html}
}

@misc{Yun2021,
  title = {Re-Labeling {{ImageNet}}: From {{Single}} to {{Multi-Labels}}, from {{Global}} to {{Localized Labels}}},
  shorttitle = {Re-Labeling {{ImageNet}}},
  author = {Yun, Sangdoo and Oh, Seong Joon and Heo, Byeongho and Han, Dongyoon and Choe, Junsuk and Chun, Sanghyuk},
  year = {2021},
  month = jul,
  number = {arXiv:2101.05022},
  eprint = {2101.05022},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2101.05022},
  urldate = {2022-12-04},
  abstract = {ImageNet has been arguably the most popular image classification benchmark, but it is also the one with a significant level of label noise. Recent studies have shown that many samples contain multiple classes, despite being assumed to be a single-label benchmark. They have thus proposed to turn ImageNet evaluation into a multi-label task, with exhaustive multi-label annotations per image. However, they have not fixed the training set, presumably because of a formidable annotation cost. We argue that the mismatch between single-label annotations and effectively multi-label images is equally, if not more, problematic in the training setup, where random crops are applied. With the single-label annotations, a random crop of an image may contain an entirely different object from the ground truth, introducing noisy or even incorrect supervision during training. We thus re-label the ImageNet training set with multi-labels. We address the annotation cost barrier by letting a strong image classifier, trained on an extra source of data, generate the multi-labels. We utilize the pixel-wise multi-label predictions before the final pooling layer, in order to exploit the additional location-specific supervision signals. Training on the re-labeled samples results in improved model performances across the board. ResNet-50 attains the top-1 classification accuracy of 78.9\% on ImageNet with our localized multi-labels, which can be further boosted to 80.2\% with the CutMix regularization. We show that the models trained with localized multi-labels also outperforms the baselines on transfer learning to object detection and instance segmentation tasks, and various robustness benchmarks. The re-labeled ImageNet training set, pre-trained weights, and the source code are available at \{https://github.com/naver-ai/relabel\_imagenet\}.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,notion},
  file = {G\:\\내 드라이브\\Papers\\Re-labeling ImageNet - from Single to Multi-Labels, from Global to Localized_2021_Yun et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\58IUQX37\\2101.html}
}

@misc{Zhang2018,
  title = {{{LQ-Nets}}: {{Learned Quantization}} for {{Highly Accurate}} and {{Compact Deep Neural Networks}}},
  shorttitle = {{{LQ-Nets}}},
  author = {Zhang, Dongqing and Yang, Jiaolong and Ye, Dongqiangzi and Hua, Gang},
  year = {2018},
  month = jul,
  number = {arXiv:1807.10029},
  eprint = {1807.10029},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1807.10029},
  urldate = {2023-10-03},
  abstract = {Although weight and activation quantization is an effective approach for Deep Neural Network (DNN) compression and has a lot of potentials to increase inference speed leveraging bit-operations, there is still a noticeable gap in terms of prediction accuracy between the quantized model and the full-precision model. To address this gap, we propose to jointly train a quantized, bit-operation-compatible DNN and its associated quantizers, as opposed to using fixed, handcrafted quantization schemes such as uniform or logarithmic quantization. Our method for learning the quantizers applies to both network weights and activations with arbitrary-bit precision, and our quantizers are easy to train. The comprehensive experiments on CIFAR-10 and ImageNet datasets show that our method works consistently well for various network structures such as AlexNet, VGG-Net, GoogLeNet, ResNet, and DenseNet, surpassing previous quantization methods in terms of accuracy by an appreciable margin. Code available at https://github.com/Microsoft/LQ-Nets},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,notion},
  file = {G\:\\내 드라이브\\Papers\\LQ-Nets - Learned Quantization for Highly Accurate and Compact Deep Neural Networks_2018_Zhang et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\33F48E8H\\1807.html}
}

@article{Zhang2019,
  title = {Wavelet Transform},
  author = {Zhang, Dengsheng and Zhang, Dengsheng},
  year = {2019},
  journal = {Fundamentals of image data mining: Analysis, Features, Classification and Retrieval},
  pages = {35--44},
  publisher = {{Springer}},
  doi = {10.1007/978-3-030-17989-2_3},
  file = {C:\Users\giwon\Zotero\storage\LDFLPZPM\978-3-030-17989-2_3.html}
}

@inproceedings{Zhang2021,
  title = {Benchmarking Ultra-High-Definition Image Super-Resolution},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF}} International Conference on Computer Vision},
  author = {Zhang, Kaihao and Li, Dongxu and Luo, Wenhan and Ren, Wenqi and Stenger, Bj{\"o}rn and Liu, Wei and Li, Hongdong and Yang, Ming-Hsuan},
  year = {2021},
  pages = {14769--14778},
  keywords = {notion},
  file = {G\:\\내 드라이브\\Papers\\Benchmarking ultra-high-definition image super-resolution_2021_Zhang et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\TVBY3W9I\\Zhang_Benchmarking_Ultra-High-Definition_Image_Super-Resolution_ICCV_2021_paper.html}
}

@inproceedings{Zhang2021a,
  title = {Datasetgan: {{Efficient}} Labeled Data Factory with Minimal Human Effort},
  shorttitle = {Datasetgan},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Zhang, Yuxuan and Ling, Huan and Gao, Jun and Yin, Kangxue and Lafleche, Jean-Francois and Barriuso, Adela and Torralba, Antonio and Fidler, Sanja},
  year = {2021},
  pages = {10145--10155},
  keywords = {notion},
  file = {G\:\\내 드라이브\\Papers\\Datasetgan - Efficient labeled data factory with minimal human effort_2021_Zhang et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\G376F23A\\Zhang_DatasetGAN_Efficient_Labeled_Data_Factory_With_Minimal_Human_Effort_CVPR_2021_paper.html}
}

@inproceedings{Zhang2021b,
  title = {Physg: {{Inverse}} Rendering with Spherical Gaussians for Physics-Based Material Editing and Relighting},
  shorttitle = {Physg},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Zhang, Kai and Luan, Fujun and Wang, Qianqian and Bala, Kavita and Snavely, Noah},
  year = {2021},
  pages = {5453--5462},
  keywords = {notion},
  file = {G\:\\내 드라이브\\Papers\\Physg - Inverse rendering with spherical gaussians for physics-based material_2021_Zhang et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\YNUPN8YD\\Zhang_PhySG_Inverse_Rendering_With_Spherical_Gaussians_for_Physics-Based_Material_Editing_CVPR_.html}
}

@article{Zhang2022,
  title = {{{SwinFIR}}: {{Revisiting}} the {{SWINIR}} with Fast {{Fourier}} Convolution and Improved Training for Image Super-Resolution},
  shorttitle = {{{SwinFIR}}},
  author = {Zhang, Dafeng and Huang, Feiyu and Liu, Shizhuo and Wang, Xiaobing and Jin, Zhezhu},
  year = {2022},
  journal = {arXiv preprint arXiv:2208.11247},
  eprint = {2208.11247},
  archiveprefix = {arxiv},
  keywords = {notion},
  file = {G\:\\내 드라이브\\Papers\\SwinFIR - Revisiting the SWINIR with fast Fourier convolution and improved_2022_Zhang et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\QZ5AVPD5\\2208.html}
}

@misc{Zhang2023,
  title = {Adding {{Conditional Control}} to {{Text-to-Image Diffusion Models}}},
  author = {Zhang, Lvmin and Agrawala, Maneesh},
  year = {2023},
  month = feb,
  number = {arXiv:2302.05543},
  eprint = {2302.05543},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2302.05543},
  urldate = {2023-02-14},
  abstract = {We present a neural network structure, ControlNet, to control pretrained large diffusion models to support additional input conditions. The ControlNet learns task-specific conditions in an end-to-end way, and the learning is robust even when the training dataset is small ({$<$} 50k). Moreover, training a ControlNet is as fast as fine-tuning a diffusion model, and the model can be trained on a personal devices. Alternatively, if powerful computation clusters are available, the model can scale to large amounts (millions to billions) of data. We report that large diffusion models like Stable Diffusion can be augmented with ControlNets to enable conditional inputs like edge maps, segmentation maps, keypoints, etc. This may enrich the methods to control large diffusion models and further facilitate related applications.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Human-Computer Interaction,Computer Science - Multimedia,notion},
  file = {G\:\\내 드라이브\\Papers\\Adding Conditional Control to Text-to-Image Diffusion Models_2023_Zhang et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\GLJBW24R\\2302.html}
}

@inproceedings{Zhou2016,
  title = {Learning Deep Features for Discriminative Localization},
  booktitle = {Proceedings of the {{IEEE}} Conference on Computer Vision and Pattern Recognition},
  author = {Zhou, Bolei and Khosla, Aditya and Lapedriza, Agata and Oliva, Aude and Torralba, Antonio},
  year = {2016},
  pages = {2921--2929},
  keywords = {notion},
  file = {G\:\\내 드라이브\\Papers\\Learning deep features for discriminative localization_2016_Zhou et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\KDQWNXW2\\Zhou_Learning_Deep_Features_CVPR_2016_paper.html}
}

@misc{Zhou2018,
  title = {{{DoReFa-Net}}: {{Training Low Bitwidth Convolutional Neural Networks}} with {{Low Bitwidth Gradients}}},
  shorttitle = {{{DoReFa-Net}}},
  author = {Zhou, Shuchang and Wu, Yuxin and Ni, Zekun and Zhou, Xinyu and Wen, He and Zou, Yuheng},
  year = {2018},
  month = feb,
  number = {arXiv:1606.06160},
  eprint = {1606.06160},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1606.06160},
  urldate = {2023-09-10},
  abstract = {We propose DoReFa-Net, a method to train convolutional neural networks that have low bitwidth weights and activations using low bitwidth parameter gradients. In particular, during backward pass, parameter gradients are stochastically quantized to low bitwidth numbers before being propagated to convolutional layers. As convolutions during forward/backward passes can now operate on low bitwidth weights and activations/gradients respectively, DoReFa-Net can use bit convolution kernels to accelerate both training and inference. Moreover, as bit convolutions can be efficiently implemented on CPU, FPGA, ASIC and GPU, DoReFa-Net opens the way to accelerate training of low bitwidth neural network on these hardware. Our experiments on SVHN and ImageNet datasets prove that DoReFa-Net can achieve comparable prediction accuracy as 32-bit counterparts. For example, a DoReFa-Net derived from AlexNet that has 1-bit weights, 2-bit activations, can be trained from scratch using 6-bit gradients to get 46.1{\textbackslash}\% top-1 accuracy on ImageNet validation set. The DoReFa-Net AlexNet model is released publicly.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,notion},
  file = {G\:\\내 드라이브\\Papers\\DoReFa-Net - Training Low Bitwidth Convolutional Neural Networks with Low Bitwidth Gradients_2018_Zhou et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\HVPHZSL4\\1606.html}
}

@misc{Zhou2018a,
  title = {{{DoReFa-Net}}: {{Training Low Bitwidth Convolutional Neural Networks}} with {{Low Bitwidth Gradients}}},
  shorttitle = {{{DoReFa-Net}}},
  author = {Zhou, Shuchang and Wu, Yuxin and Ni, Zekun and Zhou, Xinyu and Wen, He and Zou, Yuheng},
  year = {2018},
  month = feb,
  number = {arXiv:1606.06160},
  eprint = {1606.06160},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1606.06160},
  urldate = {2023-09-04},
  abstract = {We propose DoReFa-Net, a method to train convolutional neural networks that have low bitwidth weights and activations using low bitwidth parameter gradients. In particular, during backward pass, parameter gradients are stochastically quantized to low bitwidth numbers before being propagated to convolutional layers. As convolutions during forward/backward passes can now operate on low bitwidth weights and activations/gradients respectively, DoReFa-Net can use bit convolution kernels to accelerate both training and inference. Moreover, as bit convolutions can be efficiently implemented on CPU, FPGA, ASIC and GPU, DoReFa-Net opens the way to accelerate training of low bitwidth neural network on these hardware. Our experiments on SVHN and ImageNet datasets prove that DoReFa-Net can achieve comparable prediction accuracy as 32-bit counterparts. For example, a DoReFa-Net derived from AlexNet that has 1-bit weights, 2-bit activations, can be trained from scratch using 6-bit gradients to get 46.1{\textbackslash}\% top-1 accuracy on ImageNet validation set. The DoReFa-Net AlexNet model is released publicly.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,notion},
  file = {G\:\\내 드라이브\\Papers\\DoReFa-Net - Training Low Bitwidth Convolutional Neural Networks with Low_2018_Zhou et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\VUWKGI2E\\1606.html}
}

@misc{Zhou2022,
  title = {Towards {{Robust Blind Face Restoration}} with {{Codebook Lookup Transformer}}},
  author = {Zhou, Shangchen and Chan, Kelvin C. K. and Li, Chongyi and Loy, Chen Change},
  year = {2022},
  month = oct,
  number = {arXiv:2206.11253},
  eprint = {2206.11253},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2206.11253},
  urldate = {2023-02-06},
  abstract = {Blind face restoration is a highly ill-posed problem that often requires auxiliary guidance to 1) improve the mapping from degraded inputs to desired outputs, or 2) complement high-quality details lost in the inputs. In this paper, we demonstrate that a learned discrete codebook prior in a small proxy space largely reduces the uncertainty and ambiguity of restoration mapping by casting blind face restoration as a code prediction task, while providing rich visual atoms for generating high-quality faces. Under this paradigm, we propose a Transformer-based prediction network, named CodeFormer, to model the global composition and context of the low-quality faces for code prediction, enabling the discovery of natural faces that closely approximate the target faces even when the inputs are severely degraded. To enhance the adaptiveness for different degradation, we also propose a controllable feature transformation module that allows a flexible trade-off between fidelity and quality. Thanks to the expressive codebook prior and global modeling, CodeFormer outperforms the state of the arts in both quality and fidelity, showing superior robustness to degradation. Extensive experimental results on synthetic and real-world datasets verify the effectiveness of our method.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,notion},
  file = {G\:\\내 드라이브\\Papers\\Towards Robust Blind Face Restoration with Codebook Lookup Transformer_2022_Zhou et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\AGF996QD\\2206.html}
}

@inproceedings{Zhu2017,
  title = {Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks},
  booktitle = {Proceedings of the {{IEEE}} International Conference on Computer Vision},
  author = {Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A.},
  year = {2017},
  pages = {2223--2232},
  keywords = {notion},
  file = {G\:\\내 드라이브\\Papers\\Unpaired image-to-image translation using cycle-consistent adversarial networks_2017_Zhu et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\VUVG7VKI\\Zhu_Unpaired_Image-To-Image_Translation_ICCV_2017_paper.html}
}

@misc{Zhu2020,
  title = {Unpaired {{Image-to-Image Translation}} Using {{Cycle-Consistent Adversarial Networks}}},
  author = {Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A.},
  year = {2020},
  month = aug,
  number = {arXiv:1703.10593},
  eprint = {1703.10593},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1703.10593},
  urldate = {2023-06-03},
  abstract = {Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs. However, for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain \$X\$ to a target domain \$Y\$ in the absence of paired examples. Our goal is to learn a mapping \$G: X {\textbackslash}rightarrow Y\$ such that the distribution of images from \$G(X)\$ is indistinguishable from the distribution \$Y\$ using an adversarial loss. Because this mapping is highly under-constrained, we couple it with an inverse mapping \$F: Y {\textbackslash}rightarrow X\$ and introduce a cycle consistency loss to push \$F(G(X)) {\textbackslash}approx X\$ (and vice versa). Qualitative results are presented on several tasks where paired training data does not exist, including collection style transfer, object transfiguration, season transfer, photo enhancement, etc. Quantitative comparisons against several prior methods demonstrate the superiority of our approach.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,notion},
  file = {G\:\\내 드라이브\\Papers\\Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks_2020_Zhu et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\HH53ARUF\\1703.html}
}

@misc{zotero-1112,
  title = {Marian :: {{Home}}},
  urldate = {2023-12-15},
  howpublished = {https://marian-nmt.github.io/},
  file = {C:\Users\giwon\Zotero\storage\PJKYPDI4\marian-nmt.github.io.html}
}

@misc{zotero-1113,
  title = {Marian :: {{Home}}},
  urldate = {2023-12-15},
  howpublished = {https://marian-nmt.github.io/},
  file = {C:\Users\giwon\Zotero\storage\S7WRV4JY\marian-nmt.github.io.html}
}

@misc{zotero-246,
  title = {Three Types of Road Maintenance Strategies - {{Google}} 검색},
  urldate = {2023-02-10},
  howpublished = {https://www.google.com/search?q=three+types+of+road+maintenance+strategies\&ei=Zz3mY87NKtTh2roPi-O0uAM\&oq=three+road+maint\&gs\_lcp=Cgxnd3Mtd2l6LXNlcnAQAxgAMgkIABAIEB4Q8QQyCQgAEAgQHhDxBDoRCC4QgAQQsQMQgwEQxwEQ0QM6CwgAEIAEELEDEIMBOgsILhCABBDHARDRAzoHCC4Q1AIQQzoECAAQQzoECC4QQzoRCC4QgAQQsQMQgwEQxwEQrwE6BQgAEIAEOgsILhCABBCxAxCDAToICAAQgAQQsQM6EAgAEIAEELEDEIMBEEYQ\_wE6CggAELEDEIMBEEM6BAgAEAM6BQguEIAEOggILhCABBDUAjoHCAAQHhDxBDoHCAAQgAQQEzoJCAAQgAQQChATOgsIABAIEB4Q8QQQEzoICAAQCBAeEBM6BggAEAgQHjoICAAQCBAeEA86CggAEAgQHhAPEAo6CAgAEAgQHhAKSgQIQRgASgQIRhgAUABY54oBYKGaAWgPcAF4AIABhgKIAfMUkgEGMC4xNi4xmAEAoAEBwAEB\&sclient=gws-wiz-serp},
  keywords = {notion},
  file = {C:\Users\giwon\Zotero\storage\UZ8UVU53\search.html}
}

@misc{zotero-269,
  title = {Unsupervised {{Domain Adaptation}} for {{Semantic Segmentation}}... - {{Google}} 학술 검색},
  urldate = {2023-01-19},
  howpublished = {https://scholar.google.co.kr/scholar?hl=ko\&as\_sdt=0\%2C5\&q=Unsupervised+Domain+Adaptation+for+Semantic+Segmentation+via+Class-Balanced+Self-Training\&btnG=},
  keywords = {notion},
  file = {C:\Users\giwon\Zotero\storage\IGQA4UER\scholar.html}
}

@misc{zotero-277,
  title = {Probability and {{Information Theory}} - {{Google}} 검색},
  urldate = {2023-01-10},
  howpublished = {https://www.google.com/search?q=Probability+and+Information+Theory\&oq=Probability+and+Information+Theory\&aqs=chrome..69i57j0i512l2j0i30j0i5i30.387j0j7\&sourceid=chrome\&ie=UTF-8},
  keywords = {notion},
  file = {C:\Users\giwon\Zotero\storage\A9H9PHJ8\search.html}
}

@misc{zotero-278,
  urldate = {2023-01-10},
  howpublished = {https://www.deeplearningbook.org/contents/notation.html},
  keywords = {notion},
  file = {C:\Users\giwon\Zotero\storage\HTACHSSL\notation.html}
}

@misc{zotero-42,
  title = {{{BIM}}+handbook+기본편 Ch7.Pdf},
  file = {G:\내 드라이브\Papers\BIM+handbook+기본편 ch7.pdf_.pdf}
}

@misc{zotero-469,
  title = {Papers with {{Code}} - {{GPT-4 Technical Report}}},
  urldate = {2023-03-16},
  abstract = {🏆 SOTA for Multi-task Language Understanding on MMLU (Average (\%) metric)},
  howpublished = {https://paperswithcode.com/paper/gpt-4-technical-report-1},
  langid = {english},
  keywords = {notion},
  file = {C:\Users\giwon\Zotero\storage\WDSESFJG\gpt-4-technical-report-1.html}
}

@misc{zotero-798,
  urldate = {2023-10-04},
  howpublished = {https://web.archive.org/web/20120501152722/http://jmlr.csail.mit.edu/papers/v3/blei03a.html},
  keywords = {notion},
  file = {C:\Users\giwon\Zotero\storage\483EQXJM\blei03a.html}
}

@inproceedings{Zou2018,
  title = {Unsupervised Domain Adaptation for Semantic Segmentation via Class-Balanced Self-Training},
  booktitle = {Proceedings of the {{European}} Conference on Computer Vision ({{ECCV}})},
  author = {Zou, Yang and Yu, Zhiding and Kumar, B. V. K. and Wang, Jinsong},
  year = {2018},
  pages = {289--305},
  keywords = {notion},
  file = {G\:\\내 드라이브\\Papers\\Unsupervised domain adaptation for semantic segmentation via class-balanced_2018_Zou et al.pdf;C\:\\Users\\giwon\\Zotero\\storage\\ENS57WTF\\Yang_Zou_Unsupervised_Domain_Adaptation_ECCV_2018_paper.html}
}
